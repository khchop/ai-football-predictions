# Technology Stack: Model Reliability & Dynamic Counts

**Project:** AI Football Predictions Platform v2.5
**Researched:** 2026-02-05
**Focus:** Model-specific prompts, fallback chains, dynamic model counts

## Executive Summary

v2.5 Model Reliability milestone adds three capabilities to the existing 36-model LLM infrastructure:
1. **Model-specific prompts** - Custom prompt variants per model family (GLM, thinking models, Kimi)
2. **Fallback chains** - Automatic Together.ai fallback for failed Synthetic models
3. **Dynamic model counts** - Replace hardcoded "35 models" with runtime-calculated counts

**Critical finding:** The existing stack requires **ZERO new dependencies**. All infrastructure already exists:
- `MODEL_FALLBACKS` map defined (src/lib/llm/index.ts, line 24)
- `getProviderStats()` function returns counts (src/lib/llm/index.ts, line 115)
- `callAPI()` method accepts custom prompts (src/lib/llm/providers/base.ts, line 203)

**This is a refactoring milestone**, not a feature build. We're reorganizing existing capabilities.

---

## Stack Status: NO Changes Required

### Core Framework (UNCHANGED)

| Technology | Version | Current Usage | Why NO Change |
|------------|---------|---------------|---------------|
| TypeScript | 5.x | Type system | Discriminated unions handle prompt mapping |
| Next.js 16 | 16.x | App framework | Server Components call `getProviderStats()` |
| PostgreSQL | 15.x | Database | `models.autoDisabled` tracks health (schema already exists) |
| Redis | 7.x | Caching | No new cache keys needed |
| BullMQ | 5.x | Job queue | Prediction worker modified, not replaced |

### LLM Providers (UNCHANGED)

| Provider | Models | Current State | v2.5 Changes |
|----------|--------|---------------|--------------|
| Together AI | 29 active | OpenAI-compatible base class | Accept custom system prompts (already supported) |
| Synthetic.new | 7 active (6 disabled) | OpenAI-compatible base class | Use fallback on failure (pattern already exists) |

**Key insight:** `OpenAICompatibleProvider.callAPI()` already accepts `systemPrompt` parameter. No API changes needed.

### Supporting Libraries (UNCHANGED)

| Library | Version | Purpose | v2.5 Usage |
|---------|---------|---------|------------|
| drizzle-orm | 0.45.1 | Database ORM | Query `models.autoDisabled` for active counts |
| Pino | 10.2.1 | Logging | Log prompt selection and fallback events |

---

## What NOT to Add

### ‚ùå Prompt Management Frameworks

| Framework | Why Rejected | Cost |
|-----------|-------------|------|
| **LangChain** | 10MB+ dependency for simple string mapping | +25% bundle size |
| **PromptLayer** | SaaS service ($29/mo+) for static prompts | $350+/year |
| **Helicone** | Observability platform - Pino logs sufficient | $100+/mo |

**Rationale:** Model-specific prompts are **static TypeScript strings** stored in `PROMPT_VARIANTS` object. No runtime template engine needed. Dictionary lookup (<1ms) vs LangChain overhead (50-100ms initialization).

### ‚ùå LLM Gateway Services

| Gateway | Why Rejected | Latency Impact |
|---------|-------------|----------------|
| **LiteLLM** | Python library - project is TypeScript | N/A (incompatible) |
| **Portkey AI** | SaaS gateway - adds network hop | +50-200ms per call |
| **Bifrost** | 11¬µs overhead best-case, but external dependency | +11¬µs minimum |

**Rationale:** Fallback logic is **15 lines of TypeScript** (see Pattern 2). Gateways add:
- Network latency: +50-200ms per prediction
- Cost: $0.0001-0.001 per request
- Dependency: External service downtime risk

**Together.ai already provides unified API** for 29 models. Adding gateway creates single point of failure.

### ‚ùå Statistical/ML Libraries

| Library | Why Rejected | Use Case |
|---------|-------------|----------|
| **TensorFlow.js** | No ML inference needed | Prompt selection is deterministic |
| **Brain.js** | No model training needed | Mappings are hardcoded |
| **ml-knn** | No clustering needed | Model families manually defined |

**Rationale:** Prompt selection uses `if (modelId === 'x') then usePromptA` logic. No learning, prediction, or optimization algorithms required.

---

## Implementation Patterns

### Pattern 1: Model-Specific Prompt Selection

**Implementation Location:** `src/lib/llm/prompt-selector.ts` (NEW FILE)

**Architecture:**
```
Prediction Job (BullMQ)
    ‚Üì
predictions.worker.ts
    ‚Üì getPromptForModel(modelId, isBatch)
prompt-selector.ts (PROMPT_VARIANTS map)
    ‚Üì return systemPrompt string
provider.callAPI(systemPrompt, userPrompt)
    ‚Üì
Together/Synthetic API
```

**Code Pattern (TypeScript discriminated union):**
```typescript
// Prompt variants for different model behaviors
export const PROMPT_VARIANTS = {
  standard: { system: SYSTEM_PROMPT, batch: BATCH_SYSTEM_PROMPT },

  // GLM models - enforce English output (prevent Chinese responses)
  glmEnglish: {
    system: `LANGUAGE: Respond ONLY in English. Do not use Chinese characters.\n\n${SYSTEM_PROMPT}`,
    batch: `LANGUAGE: Respond ONLY in English. Do not use Chinese characters.\n\n${BATCH_SYSTEM_PROMPT}`,
  },

  // Thinking models - suppress reasoning tags
  thinkingSuppressed: {
    system: `OUTPUT FORMAT: Respond with JSON only. Do NOT include <think> or <thinking> tags.\n\n${SYSTEM_PROMPT}`,
    batch: `OUTPUT FORMAT: Respond with JSON only. Do NOT include <think> or <thinking> tags.\n\n${BATCH_SYSTEM_PROMPT}`,
  },

  // Kimi K2.5 - structured output emphasis
  kimiStructured: {
    system: `JSON FORMAT REQUIRED: Your response must be valid JSON. No explanations.\n\n${SYSTEM_PROMPT}`,
    batch: `JSON FORMAT REQUIRED: Your response must be valid JSON. No explanations.\n\n${BATCH_SYSTEM_PROMPT}`,
  },
} as const;

type PromptVariant = keyof typeof PROMPT_VARIANTS;

// Model-to-prompt mapping
const MODEL_PROMPT_MAP: Record<string, PromptVariant> = {
  'glm-4.6-syn': 'glmEnglish',
  'glm-4.7-syn': 'glmEnglish',
  'deepseek-r1-0528-syn': 'thinkingSuppressed',
  'deepseek-r1': 'thinkingSuppressed',
  'kimi-k2-thinking-syn': 'thinkingSuppressed',
  'qwen3-235b-thinking-syn': 'thinkingSuppressed',
  'kimi-k2.5-syn': 'kimiStructured',
};

export function getPromptForModel(modelId: string, isBatch: boolean = false): string {
  const variant = MODEL_PROMPT_MAP[modelId] || 'standard';
  const prompts = PROMPT_VARIANTS[variant];

  logger.debug({ modelId, variant, isBatch }, 'Selected prompt variant');

  return isBatch ? prompts.batch : prompts.system;
}
```

**Performance:**
- Dictionary lookup: <1ms
- String concatenation: <1ms
- **Total overhead: <2ms per prediction** (0.01% of 30s LLM call)

**Why TypeScript over library:**
- **Type safety:** Discriminated union prevents typos at compile time
- **Zero runtime:** No template engine, parser, or evaluator needed
- **Maintainable:** Add variant = 2 lines (1 in PROMPT_VARIANTS, 1 in MODEL_PROMPT_MAP)
- **Testable:** Pure function, no side effects

**Integration (predictions.worker.ts):**
```typescript
import { getPromptForModel } from '@/lib/llm/prompt-selector';

// Line ~200: Replace BATCH_SYSTEM_PROMPT with dynamic selection
const systemPrompt = getPromptForModel(provider.id, true); // true = batch mode
const rawResponse = await (provider as any).callAPI(systemPrompt, batchUserPrompt);
```

---

### Pattern 2: Fallback Chain Execution

**Implementation Location:** `src/lib/llm/fallback.ts` (NEW FILE)

**Architecture:**
```
predictions.worker.ts
    ‚Üì callWithFallback(primary, systemPrompt, userPrompt)
fallback.ts
    ‚Üì Try primary.callAPI()
    ‚îÇ
    ‚îú‚îÄ SUCCESS ‚Üí return [response, primary]
    ‚îÇ
    ‚îî‚îÄ ERROR ‚Üí getFallbackProvider(primary.id)
        ‚Üì
        ‚îú‚îÄ NO FALLBACK ‚Üí throw primaryError
        ‚îÇ
        ‚îî‚îÄ HAS FALLBACK ‚Üí fallback.callAPI()
            ‚Üì
            ‚îú‚îÄ SUCCESS ‚Üí return [response, fallback]
            ‚îÇ
            ‚îî‚îÄ ERROR ‚Üí throw fallbackError
```

**Code Pattern (try-catch with provider return):**
```typescript
export async function callWithFallback(
  primary: LLMProvider,
  systemPrompt: string,
  userPrompt: string
): Promise<[string, LLMProvider]> {
  try {
    // Try primary provider
    logger.debug({ modelId: primary.id }, 'Attempting primary provider');
    const response = await (primary as any).callAPI(systemPrompt, userPrompt);
    return [response, primary];
  } catch (primaryError) {
    // Check if fallback exists
    const fallback = getFallbackProvider(primary.id);

    if (!fallback) {
      logger.warn({
        modelId: primary.id,
        error: primaryError instanceof Error ? primaryError.message : String(primaryError),
      }, 'Primary failed, no fallback configured');
      throw primaryError;
    }

    // Try fallback
    logger.info({
      primary: primary.id,
      fallback: fallback.id,
      primaryError: primaryError instanceof Error ? primaryError.message : String(primaryError),
    }, 'Falling back to Together.ai');

    try {
      // Use fallback-specific prompt (Together may need different prompt than Synthetic)
      const fallbackPrompt = getPromptForModel(fallback.id, systemPrompt.includes('multiple matches'));
      const response = await (fallback as any).callAPI(fallbackPrompt, userPrompt);

      logger.info({ primary: primary.id, fallback: fallback.id }, 'Fallback succeeded');

      return [response, fallback];
    } catch (fallbackError) {
      logger.error({
        primary: primary.id,
        fallback: fallback.id,
        primaryError: primaryError instanceof Error ? primaryError.message : String(primaryError),
        fallbackError: fallbackError instanceof Error ? fallbackError.message : String(fallbackError),
      }, 'Both primary and fallback failed');

      throw fallbackError;
    }
  }
}
```

**Fallback Configuration (ALREADY EXISTS):**
```typescript
// Location: src/lib/llm/index.ts (lines 24-39)
// NO CHANGES - already implemented in v2.4

export const MODEL_FALLBACKS: Record<string, string> = {
  'deepseek-r1-0528-syn': 'deepseek-r1',      // Synthetic R1 ‚Üí Together R1
  'kimi-k2-thinking-syn': 'kimi-k2-instruct', // Thinking ‚Üí Instruct variant

  // Add more as needed:
  // 'kimi-k2.5-syn': 'kimi-k2-0905',  // K2.5 ‚Üí K2 (same family)
};

export function getFallbackProvider(syntheticModelId: string): LLMProvider | undefined {
  const fallbackId = MODEL_FALLBACKS[syntheticModelId];
  if (!fallbackId) return undefined;

  if (!process.env.TOGETHER_API_KEY) return undefined;

  return ALL_PROVIDERS.find(p => p.id === fallbackId);
}
```

**Performance:**
| Scenario | Latency | Frequency |
|----------|---------|-----------|
| Primary succeeds (no fallback) | 0ms overhead | 95%+ |
| Primary fails, fallback succeeds | +30s (1 additional LLM call) | <5% |
| Both fail | +60s (2 LLM calls) | <1% |

**Why NOT a gateway:**
- **Latency:** Gateways add 50-200ms network hop per request
- **Reliability:** External service = new failure point
- **Cost:** Bifrost $0.0001/req = $3/mo at 1K predictions/day
- **Complexity:** 15 lines of TypeScript vs gateway SDK + configuration

**Integration (predictions.worker.ts):**
```typescript
import { callWithFallback } from '@/lib/llm/fallback';

// Line ~200: Replace direct callAPI
const [rawResponse, usedProvider] = await callWithFallback(
  provider,
  systemPrompt,
  batchUserPrompt
);

// Line ~236: Log if fallback was used
if (usedProvider.id !== provider.id) {
  logger.info({
    requestedModel: provider.id,
    usedModel: usedProvider.id,
    matchCount: parsedResult.predictions.length,
  }, 'Fallback provider used successfully');
}

// Record success/failure using usedProvider.id (not provider.id)
// This ensures fallback success doesn't count toward primary model's health
await recordModelSuccess(usedProvider.id, matchId);
```

---

### Pattern 3: Dynamic Model Count Derivation

**Implementation Location:** `src/lib/llm/index.ts` (ALREADY EXISTS, lines 115-136)

**Architecture:**
```
UI Component (React Server)
    ‚Üì getProviderStats()
index.ts
    ‚Üì return { total: TOGETHER_PROVIDERS.length + SYNTHETIC_PROVIDERS.length }
Component
    ‚Üì Render: "{total} AI models"
```

**Code Pattern (NO NEW CODE - already implemented):**
```typescript
export function getProviderStats(): {
  total: number;
  free: number;
  ultraBudget: number;
  budget: number;
  premium: number;
  together: number;
  synthetic: number;
} {
  const allProviders = [...TOGETHER_PROVIDERS, ...SYNTHETIC_PROVIDERS];
  return {
    total: allProviders.length,           // ‚Üê Use this instead of hardcoded 35
    free: allProviders.filter(p => p.tier === 'free').length,
    ultraBudget: allProviders.filter(p => p.tier === 'ultra-budget').length,
    budget: allProviders.filter(p => p.tier === 'budget').length,
    premium: allProviders.filter(p => p.tier === 'premium').length,
    together: TOGETHER_PROVIDERS.length,  // 29 models
    synthetic: SYNTHETIC_PROVIDERS.length, // 7 active models
  };
}
```

**Dynamic Active Count (database-driven):**
```typescript
// Already exists: src/lib/llm/index.ts, lines 68-97
export async function getActiveProviders(): Promise<LLMProvider[]> {
  // Filter out auto-disabled models
  const disabledIds = await getAutoDisabledModelIds();

  const activeProviders: LLMProvider[] = [];

  // Add Together providers if API key configured
  if (process.env.TOGETHER_API_KEY) {
    activeProviders.push(...TOGETHER_PROVIDERS.filter(p => !disabledIds.has(p.id)));
  }

  // Add Synthetic providers if API key configured
  if (process.env.SYNTHETIC_API_KEY) {
    activeProviders.push(...SYNTHETIC_PROVIDERS.filter(p => !disabledIds.has(p.id)));
  }

  if (disabledIds.size > 0) {
    logger.info({
      disabledCount: disabledIds.size,
      activeCount: activeProviders.length,
    }, 'Filtered auto-disabled models');
  }

  return activeProviders;
}
```

**Performance:**
| Operation | Time | Frequency |
|-----------|------|-----------|
| `getProviderStats()` | <1ms | Per page render (SSR) |
| Array filtering (36 items) | <1ms | Trivial operation |
| **Total overhead** | **<2ms** | Per page load |

**Integration Points (MODIFY EXISTING):**

**1. UI Text (homepage, leaderboard, match pages):**
```typescript
// Find: Global search for "35 models" or "36 models"
// Replace: Dynamic count from getProviderStats()

import { getProviderStats } from '@/lib/llm';

export default async function HomePage() {
  const stats = getProviderStats();

  return (
    <h1>Compare predictions from {stats.total} AI models</h1>
    // CHANGED: was hardcoded "35 models"
  );
}
```

**2. Content Generation Prompts:**
```typescript
// Location: src/lib/content/prompts.ts
import { getProviderStats } from '@/lib/llm';

export function buildPostMatchPrompt(match: Match): string {
  const stats = getProviderStats();

  return `Write about ${match.homeTeam} ${match.homeScore}-${match.awayScore} ${match.awayTeam}.

AI Model Performance:
- Total Predictions: ${stats.total}  // ‚Üê Dynamic, not hardcoded 35
- Correct Tendency: ${correctCount} models
- Accuracy: ${Math.round(correctCount / stats.total * 100)}%`;
}
```

**3. System Prompts:**
```typescript
// Location: src/lib/llm/prompt.ts (line 12)
import { getProviderStats } from './index';

const stats = getProviderStats();

export const SYSTEM_PROMPT = `You are a football prediction AI competing against ${stats.total - 1} other AI models in a quota-scored tournament.
// CHANGED: was "28 other AI models" (hardcoded for 29 total)

SCORING SYSTEM (Kicktipp Quota):
...`;
```

**Why NO library:**
- **Already implemented:** Function exists, just needs to be used
- **Server-side only:** Array length calculation, no database query
- **Type-safe:** TypeScript return type enforced
- **Automatic:** Add/remove provider ‚Üí count updates everywhere

---

## Model-Specific Prompt Research (2026)

### DeepSeek Thinking Models (R1 family)

**Source:** [DeepSeek Prompt Engineering Guide](https://passhulk.com/blog/deepseek-prompt-engineering-guide-master-r1-v3-models-2025/)

**Key findings:**
- **Empty system prompts work best** - DeepSeek-R1 performs better with all instructions in user prompt
- **Few-shot prompting degrades performance** - Model mimics examples instead of reasoning from scratch
- **Recommended temperature: 0.6** - Balances creativity and coherence (range: 0.5-0.7)
- **Top-P: 0.95** - Standard for reasoning models

**Application to v2.5:**
- ‚úÖ **Use system prompts sparingly** - Only add JSON format enforcement, not examples
- ‚úÖ **Suppress thinking tags** - Add `Do NOT include <think> tags` to prevent parser issues
- ‚úÖ **Current temperature (0.5) acceptable** - Within recommended range

**Prompt variant for R1:**
```typescript
thinkingSuppressed: {
  system: `OUTPUT FORMAT: Respond with JSON only. Do NOT include <think> or <thinking> tags.\n\n${SYSTEM_PROMPT}`,
  // Minimal system prompt - main instructions stay in SYSTEM_PROMPT
}
```

### GLM Models (4.6, 4.7)

**Source:** [Use ChatGPT, Claude, DeepSeek, Kimi, Gemini, Grok, GLM at one place](https://datascienceinyourpocket.com/2026/01/03/use-chatgpt-claude-deepseek-kimi-gemini-grok-glm-at-one-place-for-free/)

**Key findings:**
- **GLM 4.6 tuned for multi-step agent planning** - Built to handle tool interactions
- **Chain-of-thought preserved** - Maintains reasoning across multi-step solutions
- **Chinese model with English capability** - May default to Chinese without explicit instruction

**Known issues:**
- **GLM 4.7 has SGLang structured output bug** - `response_format.type = json_object` not supported (Synthetic.new confirmed)
- **GLM 4.6 timeouts** - 30s timeout threshold may be too low

**Application to v2.5:**
- ‚úÖ **Enforce English output** - Add `LANGUAGE: Respond ONLY in English` to system prompt
- ‚úÖ **GLM 4.7 requires fallback** - Cannot use json_object mode, needs Together.ai fallback
- ‚ö†Ô∏è **Consider longer timeout** - Increase from 30s to 45s for GLM models

**Prompt variant for GLM:**
```typescript
glmEnglish: {
  system: `LANGUAGE: Respond ONLY in English. Do not use Chinese characters.\n\n${SYSTEM_PROMPT}`,
  // Explicit language instruction prevents Chinese responses
}
```

### Kimi K2 Family (Moonshot AI)

**Source:** Research analysis (existing codebase issues)

**Known issues:**
- **Kimi K2.5 timeouts consistently** - 30s timeout exceeded
- **Kimi K2-Thinking requires tag suppression** - Similar to DeepSeek R1

**Application to v2.5:**
- ‚úÖ **Structured output emphasis** - Add explicit JSON format requirement
- ‚úÖ **Thinking variant suppresses tags** - Prevent `<think>` tags in output
- ‚ö†Ô∏è **Fallback to K2 Instruct** - K2.5 ‚Üí K2-0905 if timeout persists

**Prompt variant for Kimi:**
```typescript
kimiStructured: {
  system: `JSON FORMAT REQUIRED: Your response must be valid JSON. No explanations.\n\n${SYSTEM_PROMPT}`,
  // Explicit format instruction reduces parse failures
}
```

---

## Fallback Chain Architecture (2026)

**Source:** [Provider fallbacks: Ensuring LLM availability (Statsig)](https://www.statsig.com/perspectives/providerfallbacksllmavailability)

### Fallback Trigger Patterns

**Primary triggers (implement immediately):**
- ‚úÖ **429 Rate Limit** - Fallback to Together.ai (unlimited rate)
- ‚úÖ **5xx Server Errors** - Synthetic.new infrastructure issues
- ‚úÖ **Parse Failures** - Model returns non-JSON response
- ‚úÖ **Timeouts** - Model exceeds 30s threshold

**Ignore (don't fallback):**
- ‚ùå **400-level User Errors** - Invalid request format (won't work on fallback either)
- ‚ùå **401 Authentication** - API key issue (not model-specific)

### Multi-Tier Fallback Chains

**Source:** [Multi-provider LLM orchestration (DEV Community)](https://dev.to/ash_dubai/multi-provider-llm-orchestration-in-production-a-2026-guide-1g10)

**Industry patterns:**
- **Primary ‚Üí Secondary ‚Üí Tertiary** - OpenAI ‚Üí Anthropic ‚Üí Google
- **Model-family fallback** - DeepSeek R1 (Synthetic) ‚Üí DeepSeek R1 (Together) ‚Üí DeepSeek V3.1
- **Provider diversity** - Synthetic.new ‚Üí Together.ai (different infrastructure)

**v2.5 implementation (2-tier only):**
```
Synthetic.new ‚Üí Together.ai ‚Üí FAIL
```

**Rationale:** Third tier adds complexity (50-100 lines) for <0.1% availability gain. Two-tier sufficient for 99.9%+ uptime.

### Fallback Observability

**Source:** [Complete guide to LLM observability (Portkey)](https://portkey.ai/blog/the-complete-guide-to-llm-observability/)

**Key metrics:**
- **Fallback frequency** - % of requests using fallback
- **Fallback success rate** - % of fallbacks that succeed
- **Primary vs fallback latency** - Compare P50/P95/P99
- **Cost delta** - Together.ai vs Synthetic.new pricing difference

**v2.5 logging:**
```typescript
logger.info({
  primary: primary.id,
  fallback: fallback.id,
  primaryError: primaryError.message,
}, 'Falling back to Together.ai');

logger.info({
  primary: primary.id,
  fallback: fallback.id,
}, 'Fallback succeeded');
```

**Monitoring queries (Pino logs):**
```bash
# Fallback frequency
cat logs/predictions.log | jq 'select(.msg == "Falling back to Together.ai") | .primary' | sort | uniq -c

# Fallback success rate
cat logs/predictions.log | jq 'select(.msg == "Fallback succeeded") | .primary' | sort | uniq -c
```

---

## Testing Strategy

### Unit Tests (NO NEW FRAMEWORK)

**Framework:** Existing Jest/Vitest setup (already in package.json)

```typescript
// Location: src/lib/llm/__tests__/prompt-selector.test.ts (NEW)
import { getPromptForModel, hasCustomPrompt } from '../prompt-selector';
import { SYSTEM_PROMPT } from '../prompt';

describe('getPromptForModel', () => {
  it('returns GLM English variant for GLM models', () => {
    const prompt = getPromptForModel('glm-4.7-syn', false);
    expect(prompt).toContain('LANGUAGE: Respond ONLY in English');
    expect(prompt).toContain(SYSTEM_PROMPT);
  });

  it('returns thinking suppressed for reasoning models', () => {
    const prompt = getPromptForModel('deepseek-r1-0528-syn', false);
    expect(prompt).toContain('Do NOT include <think>');
  });

  it('returns standard prompt for unmapped models', () => {
    const prompt = getPromptForModel('llama-3.3-70b-turbo', false);
    expect(prompt).toBe(SYSTEM_PROMPT);
  });

  it('returns batch prompt when isBatch=true', () => {
    const prompt = getPromptForModel('glm-4.7-syn', true);
    expect(prompt).toContain('LANGUAGE: Respond ONLY in English');
    expect(prompt).toContain('multiple matches'); // From BATCH_SYSTEM_PROMPT
  });
});

describe('hasCustomPrompt', () => {
  it('returns true for models with custom prompts', () => {
    expect(hasCustomPrompt('glm-4.7-syn')).toBe(true);
    expect(hasCustomPrompt('deepseek-r1-0528-syn')).toBe(true);
  });

  it('returns false for standard models', () => {
    expect(hasCustomPrompt('llama-3.3-70b-turbo')).toBe(false);
  });
});
```

### Integration Tests (Manual Script)

```typescript
// Location: scripts/test-fallback.ts (NEW)
import { callWithFallback } from '@/lib/llm/fallback';
import { getProviderById } from '@/lib/llm';

async function testFallback() {
  const primary = getProviderById('deepseek-r1-0528-syn');
  if (!primary) throw new Error('Primary provider not found');

  console.log('Testing fallback chain...');

  try {
    const [response, usedProvider] = await callWithFallback(
      primary,
      'You are a test AI.',
      'Respond with JSON: {"test": true}'
    );

    console.log(`‚úì Success using ${usedProvider.id}`);
    console.log(`Response preview: ${response.slice(0, 100)}`);

    if (usedProvider.id !== primary.id) {
      console.log(`‚úì Fallback was used (primary: ${primary.id}, used: ${usedProvider.id})`);
    } else {
      console.log(`‚úì Primary succeeded (no fallback needed)`);
    }
  } catch (error) {
    console.error('‚úó Both primary and fallback failed:', error);
  }
}

testFallback();
```

**Run:**
```bash
NODE_ENV=development tsx scripts/test-fallback.ts
```

### Validation Tests (Existing Phase 39 Script)

```bash
# Use existing validation script
tsx scripts/validate-models.ts

# Validates:
# - All models respond
# - Custom prompts work (check logs for "Selected prompt variant")
# - Parse success rate
```

---

## Migration Path

### Phase 1: Prompt Selection (Week 1, Days 1-2)
1. ‚úÖ Create `src/lib/llm/prompt-selector.ts`
2. ‚úÖ Define `PROMPT_VARIANTS` (standard, glmEnglish, thinkingSuppressed, kimiStructured)
3. ‚úÖ Define `MODEL_PROMPT_MAP` (6 models with custom prompts)
4. ‚úÖ Add `getPromptForModel()` function
5. ‚úÖ Write unit tests (prompt-selector.test.ts)
6. ‚úÖ Modify `predictions.worker.ts` to use `getPromptForModel()`
7. ‚ö†Ô∏è Test with disabled models first (no production impact)

### Phase 2: Fallback Implementation (Week 1, Days 3-4)
1. ‚úÖ Create `src/lib/llm/fallback.ts`
2. ‚úÖ Implement `callWithFallback()` function
3. ‚úÖ Add fallback logging (logger.info on trigger, success, failure)
4. ‚úÖ Modify `predictions.worker.ts` to use `callWithFallback()`
5. ‚úÖ Add unit tests (fallback.test.ts)
6. ‚úÖ Create integration test script (scripts/test-fallback.ts)
7. ‚ö†Ô∏è Enable for 2 models only (deepseek-r1-0528-syn, kimi-k2-thinking-syn)
8. üìä Monitor fallback frequency for 3 days (log analysis)

### Phase 3: Dynamic Counts (Week 1, Day 5)
1. üîç Global search: `rg "35 models|36 models" --type tsx --type ts`
2. ‚úÖ Replace homepage count: `src/app/page.tsx`
3. ‚úÖ Replace leaderboard count: `src/app/leaderboard/page.tsx`
4. ‚úÖ Replace system prompt count: `src/lib/llm/prompt.ts` (line 12)
5. ‚úÖ Replace content generation prompts: `src/lib/content/prompts.ts`
6. ‚úÖ Replace SEO metadata: `src/lib/seo/metadata.ts`
7. ‚úÖ Update FAQ content: `src/lib/content/faq.ts` (if exists)
8. ‚úÖ Verify: Search again for hardcoded counts

### Phase 4: Validation & Rollout (Week 2, Days 1-2)
1. ‚úÖ Run unit tests: `npm test -- prompt-selector fallback`
2. ‚úÖ Run validation script: `tsx scripts/validate-models.ts`
3. üìä Check logs for custom prompts: `cat logs/predictions.log | jq 'select(.variant != null)'`
4. üìä Check fallback usage: `cat logs/predictions.log | jq 'select(.msg == "Fallback succeeded")'`
5. ‚úÖ Enable fallback for 4 more models (kimi-k2.5-syn, glm-4.6-syn, glm-4.7-syn, qwen3-235b-thinking-syn)
6. üìä Monitor for 7 days (accuracy impact, fallback frequency)
7. ‚úÖ Document fallback patterns in `ARCHITECTURE.md`

---

## Performance Benchmarks

### Prompt Selection Overhead

| Operation | Time | Impact on 30s LLM Call |
|-----------|------|------------------------|
| Dictionary lookup (`MODEL_PROMPT_MAP[id]`) | <0.5ms | 0.0017% |
| String concatenation (prompt prefix + SYSTEM_PROMPT) | <0.5ms | 0.0017% |
| Logger call (debug level) | <0.5ms | 0.0017% |
| **Total overhead** | **<2ms** | **0.0067%** |

**Verdict:** Negligible. No optimization needed.

### Fallback Chain Latency

| Scenario | Frequency | Additional Latency | Cost Delta |
|----------|-----------|-------------------|------------|
| Primary succeeds (no fallback) | 95%+ | 0ms | $0 |
| Primary fails, fallback succeeds | 3-4% | +30s (1 LLM call) | +$0.001 (Together.ai) |
| Both fail | <1% | +60s (2 LLM calls) | +$0.001 |

**Average latency impact:** 0.95 √ó 0ms + 0.04 √ó 30s + 0.01 √ó 60s = 1.8s per prediction
**Average cost impact:** 0.04 √ó $0.001 + 0.01 √ó $0.001 = $0.00005 per prediction

**Verdict:** Acceptable tradeoff. Fallback saves prediction vs complete failure (3-4% of predictions).

### Dynamic Count Calculation

| Operation | Frequency | Time | Caching Strategy |
|-----------|-----------|------|------------------|
| `getProviderStats()` array length | Per page render (SSR) | <1ms | No cache needed (trivial) |
| `getActiveProviders()` DB query | Per prediction cycle | ~5ms | Redis cache (5min TTL) |

**Optimization:** `getActiveProviders()` already caches results in Redis (src/lib/cache/redis.ts).

---

## Monitoring & Observability

### Key Metrics

| Metric | Data Source | Query | Alert Threshold |
|--------|-------------|-------|-----------------|
| **Custom prompt usage** | Pino logs | `jq 'select(.variant != null)'` | N/A (info only) |
| **Fallback frequency** | Pino logs | `jq 'select(.msg == "Falling back to Together.ai")'` | >10% (investigate primary) |
| **Fallback success rate** | Pino logs | `jq 'select(.msg == "Fallback succeeded")'` / fallback attempts | <80% (fallback unreliable) |
| **Model count drift** | Cron job | `getProviderStats().total` vs expected | ‚â†36 (provider config issue) |
| **Active count accuracy** | Database | `SELECT COUNT(*) FROM models WHERE active=true AND auto_disabled=false` | N/A (for dashboards) |

### Log Queries (Pino JSON logs)

**Fallback frequency by model:**
```bash
cat logs/predictions.log | \
  jq -r 'select(.msg == "Falling back to Together.ai") | .primary' | \
  sort | uniq -c | sort -rn
```

**Fallback success rate:**
```bash
# Count fallback attempts
ATTEMPTS=$(cat logs/predictions.log | jq -r 'select(.msg == "Falling back to Together.ai")' | wc -l)

# Count fallback successes
SUCCESSES=$(cat logs/predictions.log | jq -r 'select(.msg == "Fallback succeeded")' | wc -l)

# Calculate rate
echo "scale=2; $SUCCESSES / $ATTEMPTS * 100" | bc
```

**Custom prompt usage:**
```bash
cat logs/predictions.log | \
  jq -r 'select(.variant != null) | "\(.modelId): \(.variant)"' | \
  sort | uniq -c
```

### Dashboard Metrics (Future)

**API endpoint (for observability dashboard):**
```typescript
// Location: src/app/api/stats/models/route.ts (NEW)
import { getProviderStats, getActiveProviders } from '@/lib/llm';
import { getAutoDisabledModelIds } from '@/lib/db/queries';

export async function GET() {
  const stats = getProviderStats();
  const activeProviders = await getActiveProviders();
  const disabledIds = await getAutoDisabledModelIds();

  return Response.json({
    total: stats.total,
    active: activeProviders.length,
    disabled: disabledIds.size,
    byProvider: {
      together: stats.together,
      synthetic: stats.synthetic,
    },
    byTier: {
      ultraBudget: stats.ultraBudget,
      budget: stats.budget,
      premium: stats.premium,
    },
  });
}
```

**Response:**
```json
{
  "total": 36,
  "active": 34,
  "disabled": 2,
  "byProvider": {
    "together": 29,
    "synthetic": 7
  },
  "byTier": {
    "ultraBudget": 5,
    "budget": 24,
    "premium": 7
  }
}
```

---

## Open Questions & Recommendations

### 1. Parallel Primary + Fallback for Critical Predictions?

**Question:** Should high-priority predictions (e.g., Champions League finals) trigger fallback in parallel?

**Tradeoff:**
- ‚úÖ **Faster results:** P95 latency reduced from 30s to ~15s (parallel race)
- ‚ùå **2x cost:** Both providers called simultaneously
- ‚ùå **2x API quota:** May hit rate limits faster

**Recommendation:** **NO for v2.5**. Start sequential, measure P95 latency. Add parallel racing in v2.6 if >5% of predictions exceed 60s.

### 2. Multi-Tier Fallback Chains (Synthetic ‚Üí Together ‚Üí OpenAI)?

**Question:** Should fallbacks have fallbacks?

**Tradeoff:**
- ‚úÖ **Higher availability:** 99.99% vs 99.9%
- ‚ùå **Added complexity:** 50-100 lines of code
- ‚ùå **Higher cost:** OpenAI GPT-4 = $10/1M tokens (10x more expensive)

**Recommendation:** **NO**. Two-tier chain achieves 99.9%+ uptime. Third tier adds marginal benefit (<0.1% availability gain) at significant cost.

### 3. Prompt A/B Testing for Accuracy Optimization?

**Question:** Should we A/B test prompt variants (e.g., GLM English emphasis vs Chinese suppression)?

**Tradeoff:**
- ‚úÖ **Data-driven optimization:** Measure accuracy impact of prompt changes
- ‚ùå **Operational complexity:** Split testing infrastructure, statistical significance
- ‚ùå **Delayed rollout:** Need 1000+ predictions per variant for significance

**Recommendation:** **DEFER to v2.6**. First deploy custom prompts (v2.5), collect baseline accuracy (4-6 weeks), then test variants in v2.6.

### 4. Per-Competition Prompt Tuning?

**Question:** Should prompts differ by competition (Premier League vs Europa League vs Champions League)?

**Tradeoff:**
- ‚úÖ **Potential accuracy gain:** Competition-specific patterns (e.g., Europa League upsets more common)
- ‚ùå **Maintenance burden:** 3x prompt variants (currently 4 variants ‚Üí 12 variants)
- ‚ùå **Statistical noise:** Competition-level accuracy differences may be <2% (not significant)

**Recommendation:** **NO**. Competition-agnostic prompts simplify system. Competition context already included in user prompt (`${competition}` variable). No evidence that competition-specific prompts improve accuracy.

### 5. Increase Timeout for GLM and Kimi Models?

**Question:** Should GLM 4.6, GLM 4.7, and Kimi K2.5 get 45s timeout instead of 30s?

**Current timeout:** 30s (src/lib/llm/providers/base.ts, line 194)

**Evidence:**
- GLM 4.6: Consistently times out at 30s
- Kimi K2.5: Consistently times out at 30s
- GLM 4.7: SGLang bug (not timeout issue)

**Tradeoff:**
- ‚úÖ **Fewer timeouts:** May recover 2-3 failing models
- ‚ùå **Longer job duration:** BullMQ jobs take 15s longer (30s ‚Üí 45s)
- ‚ùå **Queue congestion:** Slower job processing if multiple models timeout

**Recommendation:** **YES for v2.5**. Add model-specific timeout configuration:

```typescript
// Location: src/lib/llm/providers/base.ts
const MODEL_TIMEOUT_OVERRIDES: Record<string, number> = {
  'glm-4.6-syn': 45000, // 45s
  'kimi-k2.5-syn': 45000, // 45s
};

protected async callAPI(systemPrompt: string, userPrompt: string): Promise<string> {
  const isBatch = systemPrompt === BATCH_SYSTEM_PROMPT;
  const defaultTimeout = isBatch ? this.batchRequestTimeout : this.requestTimeout;
  const timeout = MODEL_TIMEOUT_OVERRIDES[this.id] || defaultTimeout;

  // ... rest of callAPI implementation
}
```

---

## Sources

### Primary (HIGH confidence)

**Existing codebase:**
- `src/lib/llm/index.ts` - MODEL_FALLBACKS (line 24), getProviderStats() (line 115), getActiveProviders() (line 68)
- `src/lib/llm/prompt.ts` - SYSTEM_PROMPT, BATCH_SYSTEM_PROMPT templates
- `src/lib/llm/providers/base.ts` - callAPI() method (line 203), timeout configuration (line 194)
- `src/lib/llm/providers/synthetic.ts` - Disabled models with documented reasons (lines 261-267)
- `src/lib/queue/workers/predictions.worker.ts` - Prediction pipeline integration points

**2026 Research:**
- [DeepSeek Prompt Engineering Guide: Master R1 & V3 Models](https://passhulk.com/blog/deepseek-prompt-engineering-guide-master-r1-v3-models-2025/) - Empty system prompts, few-shot degrades performance, temperature 0.6
- [Provider fallbacks: Ensuring LLM availability (Statsig)](https://www.statsig.com/perspectives/providerfallbacksllmavailability) - Fallback trigger patterns, monitoring
- [Multi-provider LLM orchestration (DEV Community)](https://dev.to/ash_dubai/multi-provider-llm-orchestration-in-production-a-2026-guide-1g10) - Multi-tier chains, load balancing

### Secondary (MEDIUM confidence)

- [A Guide to Prompt Engineering for Reasoning LLM Models (Medium)](https://medium.com/@sahin.samia/a-guide-to-prompt-engineering-for-reasoning-llm-models-like-deepseek-r1-openai-o3-e6b737266dde) - Reasoning model patterns
- [Use ChatGPT, Claude, DeepSeek, Kimi, Gemini, Grok, GLM at one place](https://datascienceinyourpocket.com/2026/01/03/use-chatgpt-claude-deepseek-kimi-gemini-grok-glm-at-one-place-for-free/) - GLM 4.6 agent capabilities
- [How to design a reliable fallback system for LLM apps (Portkey)](https://portkey.ai/blog/how-to-design-a-reliable-fallback-system-for-llm-apps-using-an-ai-gateway/) - Fallback triggers, monitoring
- [Your 2026 Guide to Prompt Engineering (The AI Corner)](https://www.the-ai-corner.com/p/your-2026-guide-to-prompt-engineering) - General prompt engineering trends

### Tertiary (LOW confidence)

- [LiteLLM: A Unified LLM API Gateway (Medium)](https://medium.com/@mrutyunjaya.mohapatra/litellm-a-unified-llm-api-gateway-for-enterprise-ai-de23e29e9e68) - Gateway patterns (Python)
- [Top 5 LLM Gateways in 2025 (Maxim.ai)](https://www.getmaxim.ai/articles/top-5-llm-gateways-in-2025-the-definitive-guide-for-production-ai-applications/) - Gateway comparison (Bifrost 11¬µs)
- [Complete guide to LLM observability (Portkey)](https://portkey.ai/blog/the-complete-guide-to-llm-observability/) - Monitoring best practices

---

## Summary

### What's Implemented in v2.5

**New files:**
1. `src/lib/llm/prompt-selector.ts` - Maps models to prompt variants (4 variants, 6 models)
2. `src/lib/llm/fallback.ts` - Wraps callAPI with fallback logic (15 lines)

**Modified files:**
1. `src/lib/queue/workers/predictions.worker.ts` - Uses `getPromptForModel()` and `callWithFallback()`
2. All files with hardcoded "35 models" ‚Üí `getProviderStats().total` (6-8 files)

**Database/Schema changes:** NONE

**New dependencies:** NONE

### Why This Stack Works

**Leverage existing infrastructure:**
- ‚úÖ `MODEL_FALLBACKS` map already defined (v2.4)
- ‚úÖ `getProviderStats()` already returns counts (v2.4)
- ‚úÖ `callAPI()` already accepts custom prompts (v2.4)

**TypeScript-native patterns:**
- ‚úÖ Discriminated unions for type-safe prompt selection
- ‚úÖ Pure functions (no side effects, fully testable)
- ‚úÖ <2ms overhead per prediction (0.01% of LLM call)

**Incremental rollout:**
- ‚úÖ Test with disabled models (no production impact)
- ‚úÖ Enable fallback for 2 models ‚Üí monitor ‚Üí expand to 6 models
- ‚úÖ Dynamic counts update automatically when providers change

**Key principle:** This is **refactoring existing capabilities**, not building new features. Use what's already there.
