---
phase: 53-regression-protection
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/generate-golden-fixtures.ts
  - src/__tests__/fixtures/golden/index.ts
  - src/__tests__/integration/models/regression.test.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "Running `npm run test:regression` executes golden fixture regression tests without API keys"
    - "Golden fixtures capture known-good response structure for each model"
    - "Regression tests validate parsing and schema code against fixtures (structure, not exact scores)"
    - "Tests detect when schema or parser changes break expected output format"
  artifacts:
    - path: "scripts/generate-golden-fixtures.ts"
      provides: "Script to capture baseline responses from all 42 models"
    - path: "src/__tests__/fixtures/golden/index.ts"
      provides: "Golden fixture index exporting fixture data by model ID"
    - path: "src/__tests__/integration/models/regression.test.ts"
      provides: "Regression test suite using golden fixtures"
  key_links:
    - from: "src/__tests__/integration/models/regression.test.ts"
      to: "src/__tests__/fixtures/golden/index.ts"
      via: "import goldenFixtures"
      pattern: "import.*golden"
    - from: "src/__tests__/integration/models/regression.test.ts"
      to: "src/__tests__/schemas/prediction.ts"
      via: "import PredictionOutputSchema"
      pattern: "PredictionOutputSchema"
    - from: "package.json"
      to: "regression.test.ts"
      via: "test:regression script"
      pattern: "test:regression"
---

<objective>
Create a regression test suite with golden fixtures that validates all 42 LLM models maintain valid JSON output. The golden fixtures capture known-good response structures so parser/schema changes can be tested offline without API calls.

Purpose: Prevent whack-a-mole regression where fixing one model's config breaks another. This is the safety net for all subsequent phases (54-58) that modify model configs.
Output: Golden fixture files for all models, regression test suite, `test:regression` npm script.
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/53-regression-protection/53-RESEARCH.md

@src/__tests__/integration/models/all-models.test.ts
@src/__tests__/schemas/prediction.ts
@src/__tests__/fixtures/test-data.ts
@src/lib/llm/prompt.ts
@vitest.config.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create golden fixture generator and fixture data</name>
  <files>
    scripts/generate-golden-fixtures.ts
    src/__tests__/fixtures/golden/index.ts
  </files>
  <action>
    Create a script `scripts/generate-golden-fixtures.ts` that generates golden fixture data.

    **Generator script (`scripts/generate-golden-fixtures.ts`):**
    - Import `ALL_PROVIDERS` from `@/lib/llm` and test data from `@/__tests__/fixtures/test-data`
    - For each provider, call `provider.predictBatch(TEST_PROMPT, [TEST_MATCH_ID])`
    - Capture per-model results:
      ```typescript
      interface GoldenFixture {
        modelId: string;
        provider: 'together' | 'synthetic';
        success: boolean;
        parsed?: { match_id: string; home_score: number; away_score: number };
        rawResponseSample?: string; // First 500 chars for debugging
        errorMessage?: string;
        timestamp: string;
      }
      ```
    - Write results to `src/__tests__/fixtures/golden/all-models.json`
    - Use `REASONING_MODEL_IDS` and `getModelTimeout()` for appropriate timeouts
    - Handle errors gracefully (capture failure info without crashing)
    - Print summary: X/42 models succeeded, Y failed
    - Use `tsx` to run: `npx tsx scripts/generate-golden-fixtures.ts`

    **Fixture index (`src/__tests__/fixtures/golden/index.ts`):**
    - Import and export the JSON fixture data
    - Export a typed `GoldenFixtures` record mapping modelId to fixture data
    - Export helper: `getSuccessfulFixtures()` returns only fixtures where `success: true`
    - Export helper: `getFixtureModelIds()` returns array of model IDs with successful fixtures
    - If `all-models.json` doesn't exist yet, export empty defaults (the file is generated by running the script, but tests should handle its absence gracefully)

    **Create initial fixture data:**
    Since we can't run the generator in this plan (requires API keys), create a seed `all-models.json` with the known 42 model IDs and placeholder structure. The executor should note that `npx tsx scripts/generate-golden-fixtures.ts` should be run manually with API keys to capture real baselines.

    **Important:** Do NOT hardcode exact prediction scores in fixtures. The fixtures capture the STRUCTURE of valid responses (fields present, types correct), not specific score values.
  </action>
  <verify>
    - `npx tsx --version` confirms tsx is available
    - `ls src/__tests__/fixtures/golden/index.ts` exists
    - `ls scripts/generate-golden-fixtures.ts` exists
    - TypeScript compiles without errors: `npx tsc --noEmit scripts/generate-golden-fixtures.ts` (or verify via vitest import)
  </verify>
  <done>
    Golden fixture generator script exists and can be run with `npx tsx scripts/generate-golden-fixtures.ts`. Fixture index file exports typed fixture data with helper functions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create regression test suite and npm script</name>
  <files>
    src/__tests__/integration/models/regression.test.ts
    package.json
  </files>
  <action>
    **Regression test file (`src/__tests__/integration/models/regression.test.ts`):**

    Create a Vitest test suite that validates model output parsing against golden fixtures WITHOUT requiring API calls:

    1. **Fixture-based structural tests (REGR-01):**
       - Import `getSuccessfulFixtures()` from golden fixtures
       - Import `PredictionOutputSchema` from `@/__tests__/schemas/prediction`
       - Import `parseBatchPredictionResponse` from `@/lib/llm/prompt`
       - For each successful golden fixture, validate:
         a. The `parsed` data passes `PredictionOutputSchema.safeParse()`
         b. `home_score` is integer 0-20
         c. `away_score` is integer 0-20
         d. `match_id` is a non-empty string
       - Use `describe.each()` or `test.each()` for clean per-model output
       - Group by provider: "Together AI Models" and "Synthetic Models"

    2. **Model count validation:**
       - Test that total fixture count matches expected (42 models)
       - Test that Together models count is 29 and Synthetic is 13
       - These catch accidental model additions/removals

    3. **Parser regression tests (REGR-01):**
       - Test `parseBatchPredictionResponse` with sample raw responses from fixtures
       - Validate it still extracts predictions correctly from known-good responses
       - Include edge cases: JSON in markdown blocks, responses with thinking tags
       - These tests catch parser code changes that break working models

    **Important:** All tests run OFFLINE (no API calls). They validate that the parsing/validation code correctly handles known-good response formats. This means:
    - No `skipIf(shouldSkip)` based on API keys
    - No actual provider.predictBatch() calls
    - Tests run fast (<5 seconds total)

    **Package.json update:**
    Add script:
    ```json
    "test:regression": "vitest run src/__tests__/integration/models/regression.test.ts"
    ```

    Also add for convenience:
    ```json
    "test:regression:live": "vitest run src/__tests__/integration/models/all-models.test.ts"
    ```
    This runs the existing all-models.test.ts which makes actual API calls (the "live" regression test).
  </action>
  <verify>
    - `npm run test:regression` runs successfully (exits 0)
    - Test output shows per-model validation results
    - No API calls made during test run (verify by running without API keys set)
    - `npm run test:regression:live` is defined in package.json
  </verify>
  <done>
    `npm run test:regression` executes golden fixture regression tests without API keys. Tests validate schema parsing, model count, and structural integrity of known-good responses. Test suite runs in under 5 seconds.
  </done>
</task>

</tasks>

<verification>
1. `npm run test:regression` passes without API keys (offline fixture validation)
2. `ls scripts/generate-golden-fixtures.ts` exists
3. `ls src/__tests__/fixtures/golden/index.ts` exists
4. `ls src/__tests__/integration/models/regression.test.ts` exists
5. `grep "test:regression" package.json` shows both scripts defined
</verification>

<success_criteria>
- Golden fixture infrastructure exists and can capture model baselines
- Regression tests validate parsing code against known-good responses offline
- `test:regression` npm script works without API keys
- Test suite is fast enough to run in CI (<5 seconds)
</success_criteria>

<output>
After completion, create `.planning/phases/53-regression-protection/53-01-SUMMARY.md`
</output>
