---
phase: 35-seo-geo-content-quality
plan: 02
type: execute
wave: 2
depends_on: ["35-01"]
files_modified:
  - src/lib/content/match-content.ts
autonomous: true

must_haves:
  truths:
    - "Finished match FAQs include specific accuracy data (X of Y models)"
    - "Finished match FAQs include specific model names in answers"
    - "Upcoming match FAQs use actual prediction data from database"
    - "All FAQs contain match-specific information, not generic placeholders"
  artifacts:
    - path: "src/lib/content/match-content.ts"
      provides: "Match-specific FAQ generation with actual data"
      contains: "How accurate were AI predictions"
  key_links:
    - from: "generateFAQContent"
      to: "modelPredictions query"
      via: "actual data in FAQ answers"
      pattern: "correctTendency.*of.*totalModels"
---

<objective>
Update FAQ generation to produce match-specific questions with actual data from predictions, including an accuracy FAQ for finished matches showing "X of Y models predicted correctly."

Purpose: FAQ schema-visual parity and GEO optimization - specific data in FAQs improves AI citations by 22-37%.
Output: generateFAQContent function produces FAQs with real match data, not generic placeholders.
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/35-seo-geo-content-quality/35-RESEARCH.md
@.planning/phases/35-seo-geo-content-quality/35-01-SUMMARY.md

# Source file to modify (already updated with answer-first prompts)
@src/lib/content/match-content.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update finished match FAQ prompt with accuracy question and specific data</name>
  <files>src/lib/content/match-content.ts</files>
  <action>
In the `generateFAQContent` function, update the finished match context section (around line 701-728).

The function already queries modelPredictions and calculates correctPredictions, exactScoreHits, etc. Update the prompt to:

1. Calculate and include accuracy percentage:
```typescript
const accuracyPct = modelPredictions.length > 0
  ? ((correctPredictions.length / modelPredictions.length) * 100).toFixed(0)
  : '0';
```

2. Update the finished match context to require specific data in FAQ answers:
```
MATCH RESULT:
${match.homeTeam} ${match.homeScore} - ${match.awayScore} ${match.awayTeam}
Competition: Match page
Date: ${formattedDate}

AI MODEL ACCURACY DATA (USE THESE EXACT NUMBERS IN YOUR ANSWERS):
- Total models that predicted: ${modelPredictions.length}
- Correctly predicted result: ${correctPredictions.length} models (${accuracyPct}%)
- Predicted exact score: ${exactScoreHits.length} model(s)
${exactScoreHits.length > 0 ? `- Models with exact score: ${exactScoreHits.map(p => p.modelName).join(', ')}` : ''}

Top 3 performers by points:
${modelPredictions.slice(0, 3).map(p => `- ${p.modelName}: ${p.totalPoints ?? 0} points`).join('\n')}

Generate 5 FAQs. CRITICAL - Question #2 MUST be the accuracy question:
1. What was the final score of ${match.homeTeam} vs ${match.awayTeam}? (Include: ${match.homeScore}-${match.awayScore}, ${formattedDate})
2. How accurate were AI predictions for ${match.homeTeam} vs ${match.awayTeam}? (MUST include: "${correctPredictions.length} of ${modelPredictions.length} models (${accuracyPct}%)")
3. Which AI models performed best for this match? (Name the top 3 models with their points)
4. Did any AI model predict the exact score? (State ${exactScoreHits.length} models, name them if any)
5. How do AI football predictions work? (Brief methodology explanation)

CRITICAL: Use the EXACT numbers provided above in your answers. Do NOT use placeholders like "check the table" or "see above".
```

3. Add ENTITY NAME CONSISTENCY instruction to the prompt
  </action>
  <verify>
Read the modified function and confirm:
- accuracyPct calculation exists
- Finished match context includes "AI MODEL ACCURACY DATA"
- FAQ #2 is specified as accuracy question with X of Y format
- Prompt includes "CRITICAL: Use the EXACT numbers"
  </verify>
  <done>
Finished match FAQs require accuracy question (#2) with specific X of Y models data and named top performers.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update upcoming match FAQ prompt with specific prediction data</name>
  <files>src/lib/content/match-content.ts</files>
  <action>
In the `generateFAQContent` function, update the upcoming/live match context section (around line 729-771).

The function already calculates homeFavor, drawFavor, awayFavor, and topScores. Update the prompt to:

1. Determine the consensus prediction (most favored outcome):
```typescript
const consensusOutcome = homeFavor > awayFavor && homeFavor > drawFavor
  ? `${match.homeTeam} win`
  : awayFavor > homeFavor && awayFavor > drawFavor
  ? `${match.awayTeam} win`
  : 'draw';
const consensusCount = Math.max(homeFavor, drawFavor, awayFavor);
```

2. Update the upcoming match context to require specific data:
```
UPCOMING MATCH:
${match.homeTeam} vs ${match.awayTeam}
Kickoff: ${formattedDate} at ${formattedTime}
Venue: ${match.venue || 'TBD'}

BETTING ODDS:
- Home win: ${analysis?.oddsHome || 'N/A'}
- Draw: ${analysis?.oddsDraw || 'N/A'}
- Away win: ${analysis?.oddsAway || 'N/A'}

AI PREDICTION DATA (USE THESE EXACT NUMBERS IN YOUR ANSWERS):
- Total models predicting: ${modelPredictions.length}
- Consensus prediction: ${consensusOutcome} (${consensusCount} of ${modelPredictions.length} models)
- Home win predictions: ${homeFavor} models
- Draw predictions: ${drawFavor} models
- Away win predictions: ${awayFavor} models
- Most predicted score(s): ${topScores.map(([score, count]) => `${score} (${count} models)`).join(', ')}

Sample model predictions:
${modelPredictions.slice(0, 5).map(p => `- ${p.modelName}: ${p.predictedHome}-${p.predictedAway}`).join('\n')}

Generate 5 FAQs for this UPCOMING match:
1. When and where is ${match.homeTeam} vs ${match.awayTeam}? (Include: ${formattedDate}, ${formattedTime}, ${match.venue || 'venue TBD'})
2. What do AI models predict for ${match.homeTeam} vs ${match.awayTeam}? (MUST include: "${consensusCount} of ${modelPredictions.length} models predict ${consensusOutcome}")
3. What is the most predicted scoreline? (State the top predicted score(s) with model counts)
4. Which AI models are predicting a ${match.homeTeam} win? (Name specific models from sample list)
5. How accurate are AI football predictions? (Brief methodology, mention 35+ models)

CRITICAL: Use the EXACT numbers and model names provided above. Do NOT use generic placeholders.
```

3. Add ENTITY NAME CONSISTENCY instruction
  </action>
  <verify>
Read the modified function and confirm:
- consensusOutcome and consensusCount calculations exist
- Upcoming match context includes "AI PREDICTION DATA"
- FAQ #2 is specified as prediction question with X of Y format
- Prompt includes specific model names and score predictions
  </verify>
  <done>
Upcoming match FAQs require prediction consensus (X of Y models) and specific model names in answers.
  </done>
</task>

</tasks>

<verification>
1. `npm run build` completes without errors
2. Finished match FAQ prompt requires accuracy question with X of Y format
3. Upcoming match FAQ prompt requires consensus prediction with model count
4. Both prompts prohibit generic placeholders
</verification>

<success_criteria>
- SGEO-03: FAQ questions contain match-specific data (teams, scores, dates)
- SGEO-04: Upcoming matches generate 5 state-specific FAQ questions
- SGEO-05: Finished matches generate 5 state-specific FAQ questions
- SGEO-06: Finished match FAQ includes "How accurate were AI predictions?" with X/35 data
</success_criteria>

<output>
After completion, create `.planning/phases/35-seo-geo-content-quality/35-02-SUMMARY.md`
</output>
