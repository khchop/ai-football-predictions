---
phase: 39-testing-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/validate-synthetic-models.ts
autonomous: true

must_haves:
  truths:
    - "All 13 Synthetic models return parseable JSON predictions"
    - "Reasoning models have thinking tags correctly stripped"
    - "GLM models output in English (or flagged for review)"
    - "Validation results logged with pass/fail per model"
  artifacts:
    - path: "scripts/validate-synthetic-models.ts"
      provides: "Model validation script"
      min_lines: 150
  key_links:
    - from: "scripts/validate-synthetic-models.ts"
      to: "src/lib/llm/providers/synthetic.ts"
      via: "imports SYNTHETIC_PROVIDERS"
      pattern: "import.*SYNTHETIC_PROVIDERS"
    - from: "scripts/validate-synthetic-models.ts"
      to: "src/lib/llm/prompt.ts"
      via: "uses parseBatchPredictionResponse"
      pattern: "parseBatchPredictionResponse"
---

<objective>
Create a validation script that tests all 13 Synthetic.new models with sample predictions, verifies JSON parsing works correctly, and logs results for each model including special-case handling (thinking tags, Chinese detection).

Purpose: Validate models work before enabling them in production prediction cycles
Output: scripts/validate-synthetic-models.ts that can be run via `npx tsx scripts/validate-synthetic-models.ts`
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/39-testing-validation/39-RESEARCH.md

@src/lib/llm/providers/synthetic.ts
@src/lib/llm/providers/base.ts
@src/lib/llm/prompt.ts
@scripts/sync-models.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create model validation script</name>
  <files>scripts/validate-synthetic-models.ts</files>
  <action>
Create a TypeScript script that validates all 13 Synthetic.new models. Follow the pattern from sync-models.ts for structure:

1. **Setup and imports:**
   - Load dotenv from .env.local (same as sync-models.ts)
   - Import SYNTHETIC_PROVIDERS from src/lib/llm/providers/synthetic
   - Import BATCH_SYSTEM_PROMPT, parseBatchPredictionResponse from src/lib/llm/prompt

2. **Test prompt constant:**
   ```typescript
   const TEST_MATCH_ID = 'test-validation-001';
   const TEST_PROMPT = `Provide a prediction for 1 test match.
Match ID: ${TEST_MATCH_ID}
Home Team: Manchester United
Away Team: Liverpool
Competition: Premier League
Kickoff: 2026-02-10

Respond with JSON array containing match_id, home_score, away_score.`;
   ```

3. **Chinese detection function:**
   ```typescript
   function containsChinese(text: string): boolean {
     return /[\u3400-\u4DBF\u4E00-\u9FFF]/.test(text);
   }
   ```

4. **Constants for model classification:**
   ```typescript
   const REASONING_MODEL_IDS = new Set([
     'deepseek-r1-0528-syn',
     'kimi-k2-thinking-syn',
     'qwen3-235b-thinking-syn',
   ]);

   const GLM_MODEL_IDS = new Set(['glm-4.6-syn', 'glm-4.7-syn']);
   ```

5. **Single model validation function:**
   - Call `provider.predictBatch()` with the test prompt and TEST_MATCH_ID
   - Parse response with parseBatchPredictionResponse
   - Return result object with: modelId, success, prediction, error, rawResponsePreview (first 300 chars), flags (hasThinkingTags, hasChinese)

6. **Main validation function:**
   - Check SYNTHETIC_API_KEY exists (exit with error if missing)
   - Log "Testing X Synthetic models..."
   - Use Promise.allSettled() to test all 13 models concurrently
   - For each result, log:
     * Model ID and success/failure
     * If reasoning model: whether thinking tags were present in raw response
     * If GLM model: whether Chinese was detected
     * Prediction values if successful
     * Error message if failed
   - Print summary: X/13 models validated successfully
   - Exit with code 0 if all pass, 1 if any fail

7. **Output format:**
   ```
   Testing 13 Synthetic models...

   [deepseek-r1-0528-syn] Reasoning model
     Raw response has thinking tags: yes
     Parsed successfully: yes
     Prediction: 2-1

   [glm-4.6-syn] GLM model
     Chinese detected: no
     Parsed successfully: yes
     Prediction: 1-0

   [minimax-m2-syn] Standard model
     Parsed successfully: yes
     Prediction: 2-2

   ...

   Summary: 13/13 models validated
   ```

**Implementation notes:**
- Use provider.predictBatch() method (inherited from OpenAICompatibleProvider) - it handles the API call and returns BatchPredictionResult
- The parser automatically strips thinking tags, so check the raw response BEFORE parsing to detect them
- Log rawResponsePreview for failed models to help debugging
- Add timeout handling (30 seconds per model) using Promise.race
</action>
  <verify>
Run script with SYNTHETIC_API_KEY set:
```bash
npx tsx scripts/validate-synthetic-models.ts
```
Script should:
- Complete without throwing
- Output results for all 13 models
- Print summary line
  </verify>
  <done>
- Script exists at scripts/validate-synthetic-models.ts
- Script imports SYNTHETIC_PROVIDERS and parseBatchPredictionResponse
- Script validates all 13 models
- Script detects thinking tags in reasoning models
- Script detects Chinese in GLM models
- Script prints pass/fail summary
  </done>
</task>

<task type="auto">
  <name>Task 2: Run validation and document results</name>
  <files>scripts/validate-synthetic-models.ts</files>
  <action>
Execute the validation script and capture results:

1. **Run the script:**
   ```bash
   npx tsx scripts/validate-synthetic-models.ts 2>&1 | tee validation-output.txt
   ```

2. **Analyze results:**
   - Count successful vs failed models
   - Note any reasoning models where thinking tags were NOT stripped (potential bug)
   - Note any GLM models outputting Chinese (may need to be disabled)

3. **Handle failures (if any):**
   - If a model consistently fails with rate limit: Log as "rate limited, not model failure"
   - If a model fails with parse error: Check rawResponsePreview, consider if prompt adjustment needed
   - If GLM outputs Chinese: Log warning, consider adding language directive to system prompt

4. **Update script with findings (if needed):**
   - Add any edge case handling discovered during testing
   - Add clearer error messages for specific failure modes

5. **Clean up:**
   - Remove validation-output.txt after documenting results in SUMMARY
   - Note: Do NOT commit temporary output files
  </action>
  <verify>
- Script ran to completion
- Results show X/13 models validated
- Any failures are documented with reason
- No temporary files left uncommitted
  </verify>
  <done>
- Validation script executed successfully
- Results documented showing which models pass/fail
- Any special cases (Chinese output, parse failures) noted
- If any models fail: reason is understood (rate limit vs model issue)
  </done>
</task>

</tasks>

<verification>
1. Script exists and runs: `npx tsx scripts/validate-synthetic-models.ts`
2. All 13 Synthetic models are tested
3. Reasoning models (3) show thinking tag detection
4. GLM models (2) show Chinese detection status
5. Summary shows validation results
</verification>

<success_criteria>
1. scripts/validate-synthetic-models.ts exists and is executable via tsx
2. Script validates all 13 Synthetic.new models
3. Script correctly identifies reasoning models and checks for thinking tags
4. Script correctly identifies GLM models and checks for Chinese output
5. Script outputs clear pass/fail results per model
6. Validation results documented for production readiness decision
</success_criteria>

<output>
After completion, create `.planning/phases/39-testing-validation/39-01-SUMMARY.md`
</output>
