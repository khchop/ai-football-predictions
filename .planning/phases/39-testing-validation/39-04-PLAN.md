---
phase: 39-testing-validation
plan: 04
type: execute
wave: 2
depends_on:
  - 39-02
files_modified: []
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "First full prediction cycle completes with 7 Synthetic models included"
    - "Models appear in predictions table with provider='synthetic'"
    - "No runtime failures during batch prediction"
  artifacts: []
  key_links:
    - from: "Prediction worker"
      to: "Synthetic models"
      via: "getActiveProviders includes Synthetic"
      pattern: "SYNTHETIC_PROVIDERS"
---

<objective>
Execute a production prediction cycle with the 7 validated Synthetic models to verify they work in the actual prediction pipeline, not just the validation script.

Purpose: Confirm Synthetic models integrate correctly with the production prediction workflow
Output: Predictions from Synthetic models in the database, verification of end-to-end flow
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/39-testing-validation/39-01-SUMMARY.md
@.planning/phases/39-testing-validation/39-02-SUMMARY.md

@src/lib/queue/workers/predictions.worker.ts
@src/lib/llm/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Trigger prediction cycle and verify Synthetic models run</name>
  <files></files>
  <action>
Trigger a prediction cycle that includes Synthetic models and verify they produce predictions.

**Approach 1 - Use admin trigger (preferred):**

If admin dashboard has prediction trigger:
1. Navigate to admin dashboard
2. Trigger prediction cycle manually
3. Watch logs for Synthetic model predictions

**Approach 2 - Direct queue trigger:**

If admin trigger not available, trigger via curl or script:

```bash
# Check if there's a trigger endpoint
grep -r "trigger.*prediction" src/app/api --include="*.ts"
```

If endpoint exists:
```bash
curl -X POST http://localhost:3000/api/admin/trigger-predictions
```

**Approach 3 - Wait for scheduled run:**

If no manual trigger, let the scheduled prediction cycle run:
1. Check when next cycle is scheduled (queue setup)
2. Wait for scheduled execution
3. Monitor logs

**Verification steps:**

1. **Watch server logs for Synthetic model activity:**
   ```bash
   # Filter for synthetic provider
   grep -i "synthetic\|provider.*syn" server.log
   ```

2. **Check predictions table for Synthetic entries:**
   ```sql
   SELECT p.id, m.displayName, m.provider, p.homeScore, p.awayScore, p.createdAt
   FROM predictions p
   JOIN models m ON m.id = p.modelId
   WHERE m.provider = 'synthetic'
   ORDER BY p.createdAt DESC
   LIMIT 20;
   ```

   Or via API if available:
   ```bash
   curl http://localhost:3000/api/predictions?provider=synthetic
   ```

3. **Verify no errors in logs:**
   - No parse errors from Synthetic models
   - No timeout errors
   - Models complete successfully

**Expected outcomes:**
- 7 Synthetic models attempt predictions
- At least some predictions saved to database
- No systematic failures (individual match failures OK)
- Logs show "provider: synthetic" in prediction entries
  </action>
  <verify>
1. Prediction cycle executed (manual or scheduled)
2. Logs show Synthetic model activity
3. At least one prediction exists from Synthetic provider in database
4. No crash or systematic failure
  </verify>
  <done>
- Prediction cycle ran with Synthetic models
- Synthetic predictions appear in predictions table
- No systematic failures from 7 working models
- End-to-end integration verified
  </done>
</task>

<task type="auto">
  <name>Task 2: Document production validation results</name>
  <files></files>
  <action>
After prediction cycle completes, document the results.

1. **Query for Synthetic predictions count:**
   ```sql
   SELECT m.displayName, COUNT(*) as predictionCount
   FROM predictions p
   JOIN models m ON m.id = p.modelId
   WHERE m.provider = 'synthetic'
   GROUP BY m.displayName
   ORDER BY predictionCount DESC;
   ```

2. **Check for any failures:**
   - Review logs for errors
   - Check if any models were auto-disabled
   - Note any rate limiting

3. **Compare with Together AI predictions:**
   - Are Synthetic predictions similar to Together AI?
   - Any anomalies in prediction values?

4. **Summarize in SUMMARY.md:**
   - Number of Synthetic predictions generated
   - Which models ran successfully
   - Any issues encountered
   - Overall production readiness assessment

**Success metrics:**
- [ ] All 7 enabled Synthetic models attempted predictions
- [ ] At least 50% of attempts resulted in saved predictions
- [ ] No models auto-disabled during cycle
- [ ] No critical errors in logs
  </action>
  <verify>
1. Production prediction count documented
2. All 7 models participated (or reason noted for missing)
3. No new auto-disabled models
4. Summary captures production validation results
  </verify>
  <done>
- Synthetic model production use verified
- Prediction counts documented
- Any issues noted and explained
- Phase 39 gap closure confirmed
  </done>
</task>

</tasks>

<verification>
1. Prediction cycle completed with Synthetic models
2. Predictions exist in database with provider='synthetic'
3. No systematic failures from working models
4. Results documented for future reference
</verification>

<success_criteria>
1. At least one prediction cycle ran with Synthetic models active
2. Predictions from Synthetic models visible in database
3. No runtime crashes or systematic failures
4. Production integration confirmed working
5. Gap "First full prediction cycle completes with Synthetic models included" is closed
</success_criteria>

<output>
After completion, create `.planning/phases/39-testing-validation/39-04-SUMMARY.md`
</output>
