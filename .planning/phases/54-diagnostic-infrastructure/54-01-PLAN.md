---
phase: 54-diagnostic-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/__tests__/fixtures/golden/diverse-scenarios.ts
  - scripts/diagnostic/categorize-failure.ts
autonomous: true

must_haves:
  truths:
    - "Diverse match scenarios exist covering standard, high-scoring, low-scoring, upset, and derby situations"
    - "Failure categorization classifies errors into exactly 6 categories: timeout, parse, language, thinking-tag, api-error, empty-response"
    - "Each failure category includes a fix recommendation string"
  artifacts:
    - path: "src/__tests__/fixtures/golden/diverse-scenarios.ts"
      provides: "5-6 typed match scenarios with buildTestPrompt() helper"
      exports: ["DIVERSE_SCENARIOS", "buildTestPrompt", "DiagnosticScenario"]
    - path: "scripts/diagnostic/categorize-failure.ts"
      provides: "FailureCategory enum, CategorizedFailure interface, categorizeFailure() function, FIX_RECOMMENDATIONS map"
      exports: ["FailureCategory", "CategorizedFailure", "categorizeFailure", "FIX_RECOMMENDATIONS", "DiagnosticResult"]
  key_links:
    - from: "scripts/diagnostic/categorize-failure.ts"
      to: "src/__tests__/fixtures/test-data.ts"
      via: "imports REASONING_MODEL_IDS for timeout context"
      pattern: "REASONING_MODEL_IDS"
    - from: "src/__tests__/fixtures/golden/diverse-scenarios.ts"
      to: "src/__tests__/fixtures/test-data.ts"
      via: "follows same test prompt format pattern"
      pattern: "match_id.*home_score.*away_score"
---

<objective>
Create diverse test fixtures and failure categorization logic for diagnostic infrastructure.

Purpose: The diagnostic runner (Plan 02) needs both diverse match scenarios to test models against AND a categorization module to classify failures. These foundational pieces have no external dependencies and enable parallel development of the runner.
Output: Typed diverse scenarios with prompt builder, and a failure categorization module with 6-category taxonomy and fix recommendations.
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/54-diagnostic-infrastructure/54-RESEARCH.md
@.planning/phases/53-regression-protection/53-01-SUMMARY.md

@src/__tests__/fixtures/test-data.ts
@src/__tests__/fixtures/golden/index.ts
@src/lib/llm/response-handlers.ts
@scripts/validate-all-models.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create diverse match scenario fixtures</name>
  <files>src/__tests__/fixtures/golden/diverse-scenarios.ts</files>
  <action>
Create a typed diverse scenarios module with 5 match scenarios covering different prediction contexts. This extends the golden fixtures from Phase 53 with scenario variety for diagnostic testing.

Define a `DiagnosticScenario` interface with fields: `id` (string), `homeTeam` (string), `awayTeam` (string), `competition` (string), `description` (string).

Export a `DIVERSE_SCENARIOS` record (keyed by scenario ID) with these 5 scenarios:
1. `standard` - Everton vs Crystal Palace, Premier League, "Mid-table clash, no clear favorite"
2. `high-scoring` - Bayern Munich vs Bochum, Bundesliga, "Top team vs struggling defense, expected 4+ goals"
3. `low-scoring` - Atletico Madrid vs Getafe, La Liga, "Defensive teams, expected 0-1 goals"
4. `upset-potential` - Luton Town vs Arsenal, Premier League, "Bottom vs top, away win expected"
5. `derby` - Manchester United vs Liverpool, Premier League, "High-stakes rivalry, draws common"

Export a `buildTestPrompt(scenarioId: string)` function that takes a scenario ID and returns the prediction prompt string. The prompt format MUST match the existing pattern from `src/__tests__/fixtures/test-data.ts`:
```
Provide a prediction for 1 test match.
Match ID: diag-{scenarioId}
Home Team: {homeTeam}
Away Team: {awayTeam}
Competition: {competition}
Kickoff: 2026-02-10

Respond with JSON array containing match_id, home_score, away_score.
```

Also export a `getAllScenarioIds()` helper that returns string[] of all scenario keys.

Use the match ID prefix `diag-` to distinguish diagnostic fixtures from the existing `test-validation-001` fixture.
  </action>
  <verify>
Run `npx tsx -e "import { DIVERSE_SCENARIOS, buildTestPrompt, getAllScenarioIds } from './src/__tests__/fixtures/golden/diverse-scenarios'; console.log(Object.keys(DIVERSE_SCENARIOS).length); console.log(buildTestPrompt('standard').includes('diag-standard')); console.log(getAllScenarioIds().length);"` - should print 5, true, 5.
  </verify>
  <done>5 diverse scenarios exported with typed interface, buildTestPrompt returns formatted prompt strings with diag- prefix match IDs, getAllScenarioIds returns all 5 keys.</done>
</task>

<task type="auto">
  <name>Task 2: Create failure categorization module</name>
  <files>scripts/diagnostic/categorize-failure.ts</files>
  <action>
Create the failure categorization module that classifies LLM failures into 6 categories with fix recommendations. This is the core diagnostic logic used by the runner (Plan 02).

Define `FailureCategory` enum with 6 values: `TIMEOUT = 'timeout'`, `PARSE = 'parse'`, `LANGUAGE = 'language'`, `THINKING_TAG = 'thinking-tag'`, `API_ERROR = 'api-error'`, `EMPTY_RESPONSE = 'empty-response'`.

Define `CategorizedFailure` interface: `modelId: string`, `category: FailureCategory`, `error: string`, `rawResponse?: string` (first 300 chars max), `fix: string`.

Define `DiagnosticResult` interface: `modelId: string`, `provider: 'together' | 'synthetic'`, `success: boolean`, `prediction?: { homeScore: number; awayScore: number }`, `error?: string`, `rawResponse: string`, `category?: FailureCategory`, `durationMs: number`, `timestamp: string`, `scenarioId: string`.

Export `FIX_RECOMMENDATIONS` record mapping each FailureCategory to a specific fix string:
- TIMEOUT: "Add model to REASONING_MODEL_IDS set for 90s timeout, or increase LLM_BATCH_TIMEOUT_MS"
- PARSE: "Inspect raw response, adjust JSON extraction regex in parsePredictionResponse()"
- LANGUAGE: "Add English-only instruction to prompt variant or switch to different model"
- THINKING_TAG: "Set responseHandler: ResponseHandler.STRIP_THINKING_TAGS in model PromptConfig"
- API_ERROR: "Check API service status, reduce concurrency, or implement circuit breaker"
- EMPTY_RESPONSE: "Verify API response extraction in callAPI() method (content/reasoning/reasoning_details)"

Export `categorizeFailure(modelId: string, error: string, rawResponse: string): CategorizedFailure` function implementing the decision tree from research. CRITICAL: Check in this specific order to avoid misclassification (per Pitfall 3 in research):
1. Timeout check: error contains 'timeout' or 'Timeout after' (case-insensitive)
2. API error check: error contains '429', 'rate limit', '5xx', 'API error', 'network', '500', '502', '503'
3. Empty response check: rawResponse is falsy or rawResponse.trim().length === 0
4. Language check: rawResponse matches Chinese character regex `/[\u3400-\u4DBF\u4E00-\u9FFF]/`
5. Thinking tag check: rawResponse matches `/<think>|<thinking>|<reasoning>/i`
6. Default: PARSE category

Each return includes the fix from FIX_RECOMMENDATIONS. Truncate rawResponse to 300 chars in the returned CategorizedFailure.

Create the `scripts/diagnostic/` directory if it doesn't exist.
  </action>
  <verify>
Run `npx tsx -e "
import { categorizeFailure, FailureCategory } from './scripts/diagnostic/categorize-failure';
const t = categorizeFailure('test', 'Timeout after 90000ms', '');
console.log(t.category === FailureCategory.TIMEOUT);
const l = categorizeFailure('glm', '', '这是中文测试');
console.log(l.category === FailureCategory.LANGUAGE);
const p = categorizeFailure('test', 'invalid json', '{bad json here}');
console.log(p.category === FailureCategory.PARSE);
const e = categorizeFailure('test', '', '');
console.log(e.category === FailureCategory.EMPTY_RESPONSE);
const a = categorizeFailure('test', '429 rate limit exceeded', 'response');
console.log(a.category === FailureCategory.API_ERROR);
const th = categorizeFailure('test', 'parse error', '<think>reasoning</think>{\"a\":1}');
console.log(th.category === FailureCategory.THINKING_TAG);
"` - should print 6 true values.
  </verify>
  <done>categorizeFailure correctly classifies all 6 failure categories in the right priority order: timeout > api-error > empty-response > language > thinking-tag > parse. Each result includes modelId, category, error, truncated rawResponse, and fix recommendation.</done>
</task>

</tasks>

<verification>
1. `src/__tests__/fixtures/golden/diverse-scenarios.ts` exists and exports 5 typed scenarios
2. `scripts/diagnostic/categorize-failure.ts` exists and exports FailureCategory enum, categorizeFailure function, DiagnosticResult interface
3. All 6 failure categories are testable and return correct category + fix recommendation
4. Both modules compile without TypeScript errors: `npx tsc --noEmit scripts/diagnostic/categorize-failure.ts` (or via tsx execution)
</verification>

<success_criteria>
- 5 diverse scenarios covering standard/high-scoring/low-scoring/upset/derby match types
- buildTestPrompt() generates prompts matching existing test prompt format with diag- prefix
- categorizeFailure() correctly categorizes all 6 failure types in priority order
- Each category maps to an actionable fix recommendation string
- DiagnosticResult interface is exported for use by diagnostic runner in Plan 02
</success_criteria>

<output>
After completion, create `.planning/phases/54-diagnostic-infrastructure/54-01-SUMMARY.md`
</output>
