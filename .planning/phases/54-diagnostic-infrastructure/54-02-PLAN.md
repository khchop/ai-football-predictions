---
phase: 54-diagnostic-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["54-01"]
files_modified:
  - scripts/diagnostic/run-diagnostics.ts
  - scripts/diagnostic/generate-report.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "Running npx tsx scripts/diagnostic/run-diagnostics.ts tests all 42 models with diverse fixtures"
    - "Each model failure is automatically categorized into one of 6 categories"
    - "Raw LLM responses are saved to per-model JSON files for debugging"
    - "Per-model success rate is calculated and displayed"
    - "A markdown diagnostic report is generated showing failure breakdown by category with fix recommendations"
  artifacts:
    - path: "scripts/diagnostic/run-diagnostics.ts"
      provides: "Main diagnostic runner script"
      exports: ["runDiagnostic", "main"]
      min_lines: 80
    - path: "scripts/diagnostic/generate-report.ts"
      provides: "Markdown report generator from diagnostic results"
      exports: ["generateDiagnosticReport"]
      min_lines: 60
    - path: "package.json"
      provides: "diagnose script entry"
      contains: "diagnose"
  key_links:
    - from: "scripts/diagnostic/run-diagnostics.ts"
      to: "scripts/diagnostic/categorize-failure.ts"
      via: "imports categorizeFailure and DiagnosticResult"
      pattern: "import.*categorizeFailure.*from.*categorize-failure"
    - from: "scripts/diagnostic/run-diagnostics.ts"
      to: "src/__tests__/fixtures/golden/diverse-scenarios.ts"
      via: "imports DIVERSE_SCENARIOS and buildTestPrompt"
      pattern: "import.*DIVERSE_SCENARIOS.*from.*diverse-scenarios"
    - from: "scripts/diagnostic/run-diagnostics.ts"
      to: "src/lib/llm/index.ts"
      via: "imports ALL_PROVIDERS for model iteration"
      pattern: "import.*ALL_PROVIDERS.*from.*llm"
    - from: "scripts/diagnostic/run-diagnostics.ts"
      to: "scripts/diagnostic/generate-report.ts"
      via: "imports generateDiagnosticReport for report output"
      pattern: "import.*generateDiagnosticReport.*from.*generate-report"
    - from: "scripts/diagnostic/run-diagnostics.ts"
      to: "src/__tests__/diagnostic-results/"
      via: "writes raw responses and reports to filesystem"
      pattern: "writeFile.*diagnostic-results"
---

<objective>
Build the diagnostic runner that tests all 42 models, captures raw responses, and generates actionable diagnostic reports.

Purpose: This is the core diagnostic tool that identifies which models fail and why. It extends the existing validate-all-models.ts pattern with failure categorization (from Plan 01), raw response capture for debugging, and markdown report generation with fix recommendations. This directly fulfills DIAG-01 through DIAG-04 requirements.
Output: Runnable diagnostic script (`npm run diagnose`), per-model raw response JSON files, markdown diagnostic report.
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/54-diagnostic-infrastructure/54-RESEARCH.md
@.planning/phases/54-diagnostic-infrastructure/54-01-SUMMARY.md

@scripts/validate-all-models.ts
@scripts/diagnostic/categorize-failure.ts
@src/__tests__/fixtures/golden/diverse-scenarios.ts
@src/__tests__/fixtures/test-data.ts
@src/lib/llm/index.ts
@src/lib/llm/providers/base.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build diagnostic runner with raw response capture</name>
  <files>scripts/diagnostic/run-diagnostics.ts</files>
  <action>
Create the main diagnostic runner script modeled on the existing `scripts/validate-all-models.ts` but with these enhancements: failure categorization, raw response capture, and diverse fixture testing.

Structure:
1. Load env vars from `.env.local` using dotenv (same pattern as validate-all-models.ts)
2. Import ALL_PROVIDERS from `../../src/lib/llm`
3. Import categorizeFailure, DiagnosticResult, FailureCategory from `./categorize-failure`
4. Import DIVERSE_SCENARIOS, buildTestPrompt, getAllScenarioIds from `../../src/__tests__/fixtures/golden/diverse-scenarios`
5. Import getModelTimeout, REASONING_MODEL_IDS from `../../src/__tests__/fixtures/test-data`
6. Import generateDiagnosticReport from `./generate-report`
7. Import pLimit from `p-limit`
8. Import fs/promises (writeFile, mkdir) and path

Constants:
- `CONCURRENCY_LIMIT = 5` (same as validate-all-models.ts to avoid rate limits)
- `DEFAULT_SCENARIO = 'standard'` (use standard scenario for initial diagnostic)

Core function `runDiagnostic(provider, scenarioId)`:
- Build prompt via buildTestPrompt(scenarioId)
- Get timeout via getModelTimeout(provider.id)
- Determine providerType from provider.id (endsWith '-syn' = 'synthetic', else 'together')
- Race predictBatch against timeout (same pattern as validate-all-models.ts)
- On success: validate with PredictionOutputSchema from `../../src/__tests__/schemas/prediction`
- On validation failure: call categorizeFailure() to categorize
- On catch: call categorizeFailure() to categorize
- Return DiagnosticResult with all fields populated

IMPORTANT: Use `provider.predictBatch(prompt, [matchId])` (not callAPI) to ensure response handlers apply (per Pitfall 5 in research). The matchId is `diag-{scenarioId}`.

Raw response capture function `captureRawResponse(result: DiagnosticResult)`:
- Output dir: `src/__tests__/diagnostic-results/raw-responses/`
- Create directory recursively if needed
- Write `{modelId}.json` with full DiagnosticResult as formatted JSON
- Log capture confirmation

Main function:
1. Check API keys (TOGETHER_API_KEY, SYNTHETIC_API_KEY) - warn if missing, filter providers
2. Print test configuration (model count, concurrency, scenario)
3. Run all providers through diagnostic with p-limit concurrency control
4. Capture raw response for each result
5. Print per-model progress with PASS/FAIL(category) status
6. Calculate and print success rate
7. Call generateDiagnosticReport() with results
8. Save report to `src/__tests__/diagnostic-results/reports/diagnostic-{YYYY-MM-DD}.md`
9. Print summary with success count, failure count, per-category breakdown

Exit code: 0 always (this is diagnostic, not gating - we want the report even with failures).

Handle the case where API keys are missing gracefully: skip models for the missing provider and continue with available models. Do NOT exit(1) for missing keys - just warn and test what's available.
  </action>
  <verify>
1. File compiles: `npx tsx --eval "import './scripts/diagnostic/run-diagnostics'" 2>&1 | head -5` shows no import errors
2. Check imports resolve: The file should import from categorize-failure, diverse-scenarios, generate-report, and llm without errors
3. Verify the script is runnable (dry check, don't actually call APIs): `npx tsx -e "console.log('imports ok')" 2>&1`
  </verify>
  <done>Diagnostic runner script exists at scripts/diagnostic/run-diagnostics.ts, imports all dependencies correctly, implements concurrency-limited model testing with failure categorization, raw response capture to filesystem, and report generation. Uses predictBatch (not callAPI) to honor response handlers.</done>
</task>

<task type="auto">
  <name>Task 2: Create report generator and add npm script</name>
  <files>scripts/diagnostic/generate-report.ts, package.json</files>
  <action>
**Part A: Report Generator (scripts/diagnostic/generate-report.ts)**

Create the report generator module that takes DiagnosticResult[] and produces a formatted markdown string.

Import DiagnosticResult, FailureCategory, FIX_RECOMMENDATIONS from `./categorize-failure`.

Export `generateDiagnosticReport(results: DiagnosticResult[]): string` function that generates a markdown report with:

1. **Header**: `# Diagnostic Report - {date}`
2. **Metadata**: Generated timestamp, total models tested
3. **Summary section**:
   - Success rate as percentage: `{successCount}/{total} ({percentage}%)`
   - Passed count, Failed count
4. **Failure distribution** (if any failures):
   - List each category with count: `- timeout: 3 models`
5. **Failure Breakdown by Category** section:
   - For each category with failures:
     - Category header with count: `### TIMEOUT (3 models)`
     - Affected models: list of model IDs in backticks
     - Recommended fix: from FIX_RECOMMENDATIONS in a code block
     - Sample errors: up to 3 error messages with model ID
6. **Per-Model Results** table:
   - Columns: Model | Provider | Status | Duration | Category | Error
   - Sort by: failures first (grouped by category), then successes (sorted by duration)
   - Status: "PASS" or "FAIL"
   - Truncate error to 50 chars with "..." suffix
7. **Footer**: Raw responses path reference: `src/__tests__/diagnostic-results/raw-responses/`

Also export a helper `groupByCategory(results: DiagnosticResult[]): Record<FailureCategory, DiagnosticResult[]>` that groups failed results by their category field.

**Part B: NPM Script (package.json)**

Add a `"diagnose"` script to package.json:
```
"diagnose": "npx tsx scripts/diagnostic/run-diagnostics.ts"
```

Add it near the existing `"validate:models"` script for discoverability.

Do NOT modify any other scripts in package.json.
  </action>
  <verify>
1. Report generator compiles: `npx tsx -e "import { generateDiagnosticReport } from './scripts/diagnostic/generate-report'; console.log(typeof generateDiagnosticReport);"` prints "function"
2. Report generates valid markdown for test data: `npx tsx -e "
import { generateDiagnosticReport } from './scripts/diagnostic/generate-report';
import { FailureCategory } from './scripts/diagnostic/categorize-failure';
const results = [
  { modelId: 'test-model', provider: 'together', success: true, rawResponse: '{\"a\":1}', durationMs: 1500, timestamp: new Date().toISOString(), scenarioId: 'standard' },
  { modelId: 'fail-model', provider: 'synthetic', success: false, error: 'Timeout after 90000ms', rawResponse: '', category: FailureCategory.TIMEOUT, durationMs: 90000, timestamp: new Date().toISOString(), scenarioId: 'standard' }
];
const report = generateDiagnosticReport(results);
console.log(report.includes('Diagnostic Report'));
console.log(report.includes('50.0%'));
console.log(report.includes('TIMEOUT'));
"` prints 3 true values
3. NPM script exists: `node -e "const p = require('./package.json'); console.log(!!p.scripts.diagnose);"` prints true
  </verify>
  <done>generateDiagnosticReport produces complete markdown with summary stats, failure breakdown by category with fix recommendations, and per-model results table. Package.json has "diagnose" script that runs the diagnostic runner via tsx.</done>
</task>

</tasks>

<verification>
1. `scripts/diagnostic/run-diagnostics.ts` exists and compiles without errors
2. `scripts/diagnostic/generate-report.ts` exists and exports generateDiagnosticReport function
3. `npm run diagnose --` is a valid script (package.json updated)
4. Report generator produces markdown with: summary, failure breakdown by category, per-model table
5. Raw response capture writes to `src/__tests__/diagnostic-results/raw-responses/`
6. Report output writes to `src/__tests__/diagnostic-results/reports/`
7. All 4 requirements covered:
   - DIAG-01: Runner tests each model with golden fixture data (diverse scenarios)
   - DIAG-02: categorizeFailure classifies into 6 categories
   - DIAG-03: Per-model success rate in report summary
   - DIAG-04: Raw responses captured to filesystem
</verification>

<success_criteria>
- `npm run diagnose` runs the full diagnostic pipeline (requires API keys)
- Each model tested with concurrency limit of 5
- Failed models categorized into timeout/parse/language/thinking-tag/api-error/empty-response
- Raw responses saved as JSON files per model in diagnostic-results/raw-responses/
- Markdown report generated with:
  - Overall success rate
  - Failure counts per category
  - Fix recommendations per category
  - Per-model results table with status, duration, category, error
- Report saved to diagnostic-results/reports/ with date in filename
</success_criteria>

<output>
After completion, create `.planning/phases/54-diagnostic-infrastructure/54-02-SUMMARY.md`
</output>
