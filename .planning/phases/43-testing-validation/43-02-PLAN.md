---
phase: 43-testing-validation
plan: 02
type: execute
wave: 2
depends_on: ["43-01"]
files_modified:
  - src/__tests__/integration/models/all-models.test.ts
  - scripts/validate-all-models.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "Integration test validates JSON output for all 42 models"
    - "Test uses real APIs with proper timeout configuration"
    - "npm run validate:models executes validation and reports results"
    - "6 previously disabled models are validated with separate reporting"
    - "Previously disabled models each achieve >90% success rate"
  artifacts:
    - path: "src/__tests__/integration/models/all-models.test.ts"
      provides: "Parameterized integration tests for all providers"
      contains: "describe.each"
    - path: "scripts/validate-all-models.ts"
      provides: "Standalone validation script for CI/manual runs"
      contains: "PREVIOUSLY_DISABLED_MODELS"
    - path: "package.json"
      provides: "validate:models npm script"
      contains: "validate:models"
  key_links:
    - from: "src/__tests__/integration/models/all-models.test.ts"
      to: "src/lib/llm/index.ts"
      via: "imports ALL_PROVIDERS"
      pattern: "import.*ALL_PROVIDERS"
    - from: "src/__tests__/integration/models/all-models.test.ts"
      to: "src/__tests__/schemas/prediction.ts"
      via: "imports validation schema"
      pattern: "import.*PredictionOutputSchema"
    - from: "scripts/validate-all-models.ts"
      to: "PREVIOUSLY_DISABLED_MODELS constant"
      via: "separate reporting for 6 rehabilitated models"
      pattern: "PREVIOUSLY_DISABLED_MODELS"
---

<objective>
Create comprehensive integration tests that validate JSON output for all 42 LLM models (29 Together + 13 Synthetic), with explicit validation of the 6 previously disabled models.

Purpose: Satisfy JSON-04 requirement by testing that every model returns parseable JSON with correct structure (match_id, home_score, away_score) regardless of the specific prediction values. Additionally confirm that the 6 models re-enabled during Phases 40-41 are now functioning with >90% success rate.

Output: Working integration test suite and npm script for model validation with separate reporting for previously disabled models.
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/43-testing-validation/43-RESEARCH.md
@.planning/phases/43-testing-validation/43-01-SUMMARY.md
@scripts/validate-synthetic-models.ts
@src/lib/llm/index.ts
@src/__tests__/schemas/prediction.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create parameterized integration test for all models</name>
  <files>
    - src/__tests__/integration/models/all-models.test.ts
  </files>
  <action>
1. Create directory structure: `src/__tests__/integration/models/`

2. Create `all-models.test.ts` following Vitest parameterized test patterns:
   - Import { describe, test, expect, beforeAll } from 'vitest'
   - Import ALL_PROVIDERS from '@/lib/llm'
   - Import PredictionOutputSchema from '../../../schemas/prediction'
   - Import TEST_MATCH_ID, TEST_PROMPT, REASONING_MODEL_IDS, timeouts from '../../../fixtures/test-data'

3. Add conditional skip logic:
   ```typescript
   const hasTogetherKey = !!process.env.TOGETHER_API_KEY;
   const hasSyntheticKey = !!process.env.SYNTHETIC_API_KEY;
   const shouldSkip = !hasTogetherKey && !hasSyntheticKey;
   ```

4. Create parameterized test using describe.each:
   ```typescript
   describe.skipIf(shouldSkip)('JSON-04: All Models JSON Validation', () => {
     describe.each(ALL_PROVIDERS)('Model: $id', (provider) => {
       // Determine timeout based on model type
       const isReasoning = REASONING_MODEL_IDS.has(provider.id);
       const timeout = isReasoning ? 90000 : 60000;

       test('returns valid JSON structure', async () => {
         const result = await provider.predictBatch(TEST_PROMPT, [TEST_MATCH_ID]);

         // Assert basic success
         expect(result.success).toBe(true);
         expect(result.predictions.size).toBeGreaterThan(0);

         // Get prediction
         const prediction = result.predictions.get(TEST_MATCH_ID);
         expect(prediction).toBeDefined();

         if (prediction) {
           // Validate structure with Zod (not exact values)
           const validation = PredictionOutputSchema.safeParse({
             match_id: TEST_MATCH_ID,
             home_score: prediction.homeScore,
             away_score: prediction.awayScore,
           });

           expect(validation.success).toBe(true);

           if (!validation.success) {
             console.error(`Validation failed for ${provider.id}:`, validation.error.issues);
           }
         }
       }, { timeout, retry: 1 });
     });
   });
   ```

5. Add beforeAll hook to log test configuration:
   ```typescript
   beforeAll(() => {
     console.log(`Testing ${ALL_PROVIDERS.length} models`);
     console.log(`Together API: ${hasTogetherKey ? 'available' : 'missing'}`);
     console.log(`Synthetic API: ${hasSyntheticKey ? 'available' : 'missing'}`);
   });
   ```

Key points:
- Use describe.each for parameterization (one test per model)
- Use skipIf when no API keys present
- Set timeout per-test based on model type (90s reasoning, 60s standard)
- Use retry: 1 for transient failures
- Validate STRUCTURE not exact prediction values
  </action>
  <verify>
Run `npm run test -- --run src/__tests__/integration/models/all-models.test.ts` (may skip if no API keys in CI).
If API keys are present, at least one model should pass.
  </verify>
  <done>
Integration test file created with parameterized tests for all 42 models. Test validates JSON structure using Zod schema.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create standalone validation script with previously disabled model tracking</name>
  <files>
    - scripts/validate-all-models.ts
    - package.json
  </files>
  <action>
1. Create `scripts/validate-all-models.ts` building on existing validate-synthetic-models.ts patterns:
   - Import dotenv and load .env.local
   - Import ALL_PROVIDERS from '../src/lib/llm'
   - Import TOGETHER_PROVIDERS, SYNTHETIC_PROVIDERS for categorization
   - Import test fixtures and schemas

2. **CRITICAL: Define PREVIOUSLY_DISABLED_MODELS constant for separate tracking:**
   ```typescript
   // These 6 models were previously disabled but re-enabled during Phases 40-41
   // with model-specific prompts and fallback chains. They require explicit
   // validation to confirm >90% success rate.
   const PREVIOUSLY_DISABLED_MODELS = [
     'deepseek-r1-0528-syn',
     'kimi-k2-thinking-syn',
     'kimi-k2.5-syn',
     'glm-4.6-syn',
     'glm-4.7-syn',
     'qwen3-235b-thinking-syn',
   ] as const;
   ```

3. Implement validation with concurrent execution:
   ```typescript
   // Use p-limit for concurrency control (already in project)
   import pLimit from 'p-limit';

   const limit = pLimit(5); // Max 5 concurrent API calls

   interface ValidationResult {
     modelId: string;
     provider: 'together' | 'synthetic';
     success: boolean;
     prediction?: { homeScore: number; awayScore: number };
     error?: string;
     durationMs: number;
     wasPreviouslyDisabled: boolean;
   }

   async function validateModel(provider: LLMProvider): Promise<ValidationResult> {
     const start = Date.now();
     const isReasoning = REASONING_MODEL_IDS.has(provider.id);
     const timeout = isReasoning ? 90000 : 60000;
     const wasPreviouslyDisabled = PREVIOUSLY_DISABLED_MODELS.includes(provider.id as any);

     try {
       const result = await Promise.race([
         provider.predictBatch(TEST_PROMPT, [TEST_MATCH_ID]),
         new Promise<never>((_, reject) =>
           setTimeout(() => reject(new Error(`Timeout after ${timeout}ms`)), timeout)
         ),
       ]);

       // Validate with Zod schema
       // ... (similar to existing script)

       return {
         modelId: provider.id,
         provider: provider.id.endsWith('-syn') ? 'synthetic' : 'together',
         success: result.success,
         prediction: ...,
         durationMs: Date.now() - start,
         wasPreviouslyDisabled,
       };
     } catch (error) {
       return {
         modelId: provider.id,
         provider: ...,
         success: false,
         error: error instanceof Error ? error.message : 'Unknown',
         durationMs: Date.now() - start,
         wasPreviouslyDisabled,
       };
     }
   }
   ```

4. **CRITICAL: Main execution with separate previously disabled model reporting:**
   ```typescript
   async function main() {
     console.log('\n=== All Models Validation ===\n');
     console.log(`Testing ${ALL_PROVIDERS.length} models (${TOGETHER_PROVIDERS.length} Together + ${SYNTHETIC_PROVIDERS.length} Synthetic)\n`);

     const results = await Promise.all(
       ALL_PROVIDERS.map(p => limit(() => validateModel(p)))
     );

     // Print overall results
     const successful = results.filter(r => r.success);
     const failed = results.filter(r => !r.success);

     console.log('\n=== OVERALL RESULTS ===\n');
     console.log(`Success: ${successful.length}/${results.length}`);
     console.log(`Failed: ${failed.length}/${results.length}`);

     if (failed.length > 0) {
       console.log('\nFailed models:');
       for (const f of failed) {
         console.log(`  - ${f.modelId}: ${f.error}`);
       }
     }

     // **CRITICAL: Separate reporting for previously disabled models**
     console.log('\n=== PREVIOUSLY DISABLED MODELS (Phase 40-41 Rehabilitated) ===\n');
     const previouslyDisabledResults = results.filter(r => r.wasPreviouslyDisabled);
     const pdSuccessful = previouslyDisabledResults.filter(r => r.success);
     const pdFailed = previouslyDisabledResults.filter(r => !r.success);
     const pdSuccessRate = pdSuccessful.length / previouslyDisabledResults.length;

     console.log('Models validated:');
     for (const result of previouslyDisabledResults) {
       const status = result.success ? 'PASS' : 'FAIL';
       const details = result.success
         ? `${result.durationMs}ms`
         : result.error;
       console.log(`  ${status} ${result.modelId}: ${details}`);
     }

     console.log(`\nPreviously disabled success rate: ${pdSuccessful.length}/${previouslyDisabledResults.length} (${(pdSuccessRate * 100).toFixed(1)}%)`);

     if (pdSuccessRate < 0.90) {
       console.log(`\n!!! CRITICAL: Previously disabled models below 90% threshold !!!`);
       console.log(`Failed models from this group:`);
       for (const f of pdFailed) {
         console.log(`  - ${f.modelId}: ${f.error}`);
       }
     } else {
       console.log(`\nPreviously disabled models PASSED >90% threshold`);
     }

     // Exit code based on both overall and previously disabled success rate
     const overallSuccessRate = successful.length / results.length;

     console.log('\n=== FINAL VERDICT ===\n');

     let exitCode = 0;

     if (overallSuccessRate < 0.90) {
       console.log(`FAIL: Overall success rate ${(overallSuccessRate * 100).toFixed(1)}% < 90%`);
       exitCode = 1;
     } else {
       console.log(`PASS: Overall success rate ${(overallSuccessRate * 100).toFixed(1)}%`);
     }

     if (pdSuccessRate < 0.90) {
       console.log(`FAIL: Previously disabled models ${(pdSuccessRate * 100).toFixed(1)}% < 90%`);
       exitCode = 1;
     } else {
       console.log(`PASS: Previously disabled models ${(pdSuccessRate * 100).toFixed(1)}%`);
     }

     process.exit(exitCode);
   }
   ```

5. Add npm scripts to package.json:
   - "validate:models": "npx tsx scripts/validate-all-models.ts"
   - "validate:synthetic": "npx tsx scripts/validate-synthetic-models.ts" (keep existing)
   - "test:integration": "vitest run src/__tests__/integration/"

The script:
- Tests all 42 models (not just Synthetic)
- Uses concurrency limit (5) to avoid rate limits
- **Separately reports success/failure for the 6 previously disabled models**
- **Explicitly checks that previously disabled models achieve >90% success**
- Exits with code 1 if either overall OR previously disabled <90% success rate
  </action>
  <verify>
Run `npm run validate:models` with API keys present - should complete and report results.
Check package.json has new scripts.
Verify output includes "PREVIOUSLY DISABLED MODELS" section with pass/fail per model.
  </verify>
  <done>
Validation script created with explicit tracking and reporting for the 6 previously disabled models. Running `npm run validate:models` tests all 42 models, reports overall success rate, AND separately validates that deepseek-r1-0528-syn, kimi-k2-thinking-syn, kimi-k2.5-syn, glm-4.6-syn, glm-4.7-syn, and qwen3-235b-thinking-syn each achieve >90% success rate.
  </done>
</task>

</tasks>

<verification>
1. `npm run test -- --run src/__tests__/integration/` runs integration tests
2. `npm run validate:models` executes full validation against real APIs
3. Test output shows per-model results with JSON structure validation
4. **Output includes dedicated "PREVIOUSLY DISABLED MODELS" section**
5. **Each of the 6 previously disabled models shows individual PASS/FAIL status**
6. Script exits with code 0 if both overall >90% AND previously disabled >90%
7. Script exits with code 1 if either threshold fails
</verification>

<success_criteria>
- Integration test file uses describe.each for all 42 models
- Each test validates JSON structure with Zod schema
- Timeout configured per model type (90s reasoning, 60s standard)
- npm run validate:models runs full validation
- PREVIOUSLY_DISABLED_MODELS constant defined with 6 model IDs
- Validation script outputs separate section for previously disabled models
- Script checks >90% success rate for previously disabled models specifically
- Script fails if previously disabled models <90% even if overall passes
</success_criteria>

<output>
After completion, create `.planning/phases/43-testing-validation/43-02-SUMMARY.md`
</output>
