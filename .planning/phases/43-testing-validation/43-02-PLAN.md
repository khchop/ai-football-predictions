---
phase: 43-testing-validation
plan: 02
type: execute
wave: 2
depends_on: ["43-01"]
files_modified:
  - src/__tests__/integration/models/all-models.test.ts
  - scripts/validate-all-models.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "Integration test validates JSON output for all 42 models"
    - "Test uses real APIs with proper timeout configuration"
    - "npm run validate:models executes validation and reports results"
  artifacts:
    - path: "src/__tests__/integration/models/all-models.test.ts"
      provides: "Parameterized integration tests for all providers"
      contains: "describe.each"
    - path: "scripts/validate-all-models.ts"
      provides: "Standalone validation script for CI/manual runs"
      contains: "ALL_PROVIDERS"
    - path: "package.json"
      provides: "validate:models npm script"
      contains: "validate:models"
  key_links:
    - from: "src/__tests__/integration/models/all-models.test.ts"
      to: "src/lib/llm/index.ts"
      via: "imports ALL_PROVIDERS"
      pattern: "import.*ALL_PROVIDERS"
    - from: "src/__tests__/integration/models/all-models.test.ts"
      to: "src/__tests__/schemas/prediction.ts"
      via: "imports validation schema"
      pattern: "import.*PredictionOutputSchema"
---

<objective>
Create comprehensive integration tests that validate JSON output for all 42 LLM models (29 Together + 13 Synthetic).

Purpose: Satisfy JSON-04 requirement by testing that every model returns parseable JSON with correct structure (match_id, home_score, away_score) regardless of the specific prediction values.

Output: Working integration test suite and npm script for model validation.
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/43-testing-validation/43-RESEARCH.md
@.planning/phases/43-testing-validation/43-01-SUMMARY.md
@scripts/validate-synthetic-models.ts
@src/lib/llm/index.ts
@src/__tests__/schemas/prediction.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create parameterized integration test for all models</name>
  <files>
    - src/__tests__/integration/models/all-models.test.ts
  </files>
  <action>
1. Create directory structure: `src/__tests__/integration/models/`

2. Create `all-models.test.ts` following Vitest parameterized test patterns:
   - Import { describe, test, expect, beforeAll } from 'vitest'
   - Import ALL_PROVIDERS from '@/lib/llm'
   - Import PredictionOutputSchema from '../../../schemas/prediction'
   - Import TEST_MATCH_ID, TEST_PROMPT, REASONING_MODEL_IDS, timeouts from '../../../fixtures/test-data'

3. Add conditional skip logic:
   ```typescript
   const hasTogetherKey = !!process.env.TOGETHER_API_KEY;
   const hasSyntheticKey = !!process.env.SYNTHETIC_API_KEY;
   const shouldSkip = !hasTogetherKey && !hasSyntheticKey;
   ```

4. Create parameterized test using describe.each:
   ```typescript
   describe.skipIf(shouldSkip)('JSON-04: All Models JSON Validation', () => {
     describe.each(ALL_PROVIDERS)('Model: $id', (provider) => {
       // Determine timeout based on model type
       const isReasoning = REASONING_MODEL_IDS.has(provider.id);
       const timeout = isReasoning ? 90000 : 60000;

       test('returns valid JSON structure', async () => {
         const result = await provider.predictBatch(TEST_PROMPT, [TEST_MATCH_ID]);

         // Assert basic success
         expect(result.success).toBe(true);
         expect(result.predictions.size).toBeGreaterThan(0);

         // Get prediction
         const prediction = result.predictions.get(TEST_MATCH_ID);
         expect(prediction).toBeDefined();

         if (prediction) {
           // Validate structure with Zod (not exact values)
           const validation = PredictionOutputSchema.safeParse({
             match_id: TEST_MATCH_ID,
             home_score: prediction.homeScore,
             away_score: prediction.awayScore,
           });

           expect(validation.success).toBe(true);

           if (!validation.success) {
             console.error(`Validation failed for ${provider.id}:`, validation.error.issues);
           }
         }
       }, { timeout, retry: 1 });
     });
   });
   ```

5. Add beforeAll hook to log test configuration:
   ```typescript
   beforeAll(() => {
     console.log(`Testing ${ALL_PROVIDERS.length} models`);
     console.log(`Together API: ${hasTogetherKey ? 'available' : 'missing'}`);
     console.log(`Synthetic API: ${hasSyntheticKey ? 'available' : 'missing'}`);
   });
   ```

Key points:
- Use describe.each for parameterization (one test per model)
- Use skipIf when no API keys present
- Set timeout per-test based on model type (90s reasoning, 60s standard)
- Use retry: 1 for transient failures
- Validate STRUCTURE not exact prediction values
  </action>
  <verify>
Run `npm run test -- --run src/__tests__/integration/models/all-models.test.ts` (may skip if no API keys in CI).
If API keys are present, at least one model should pass.
  </verify>
  <done>
Integration test file created with parameterized tests for all 42 models. Test validates JSON structure using Zod schema.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create standalone validation script and npm commands</name>
  <files>
    - scripts/validate-all-models.ts
    - package.json
  </files>
  <action>
1. Create `scripts/validate-all-models.ts` building on existing validate-synthetic-models.ts patterns:
   - Import dotenv and load .env.local
   - Import ALL_PROVIDERS from '../src/lib/llm'
   - Import TOGETHER_PROVIDERS, SYNTHETIC_PROVIDERS for categorization
   - Import test fixtures and schemas

2. Implement validation with concurrent execution:
   ```typescript
   // Use p-limit for concurrency control (already in project)
   import pLimit from 'p-limit';

   const limit = pLimit(5); // Max 5 concurrent API calls

   interface ValidationResult {
     modelId: string;
     provider: 'together' | 'synthetic';
     success: boolean;
     prediction?: { homeScore: number; awayScore: number };
     error?: string;
     durationMs: number;
   }

   async function validateModel(provider: LLMProvider): Promise<ValidationResult> {
     const start = Date.now();
     const isReasoning = REASONING_MODEL_IDS.has(provider.id);
     const timeout = isReasoning ? 90000 : 60000;

     try {
       const result = await Promise.race([
         provider.predictBatch(TEST_PROMPT, [TEST_MATCH_ID]),
         new Promise<never>((_, reject) =>
           setTimeout(() => reject(new Error(`Timeout after ${timeout}ms`)), timeout)
         ),
       ]);

       // Validate with Zod schema
       // ... (similar to existing script)

       return {
         modelId: provider.id,
         provider: provider.id.endsWith('-syn') ? 'synthetic' : 'together',
         success: result.success,
         prediction: ...,
         durationMs: Date.now() - start,
       };
     } catch (error) {
       return {
         modelId: provider.id,
         provider: ...,
         success: false,
         error: error instanceof Error ? error.message : 'Unknown',
         durationMs: Date.now() - start,
       };
     }
   }
   ```

3. Main execution:
   ```typescript
   async function main() {
     console.log('\n=== All Models Validation ===\n');
     console.log(`Testing ${ALL_PROVIDERS.length} models (${TOGETHER_PROVIDERS.length} Together + ${SYNTHETIC_PROVIDERS.length} Synthetic)\n`);

     const results = await Promise.all(
       ALL_PROVIDERS.map(p => limit(() => validateModel(p)))
     );

     // Print results
     const successful = results.filter(r => r.success);
     const failed = results.filter(r => !r.success);

     console.log('\n=== RESULTS ===\n');
     console.log(`Success: ${successful.length}/${results.length}`);
     console.log(`Failed: ${failed.length}/${results.length}`);

     if (failed.length > 0) {
       console.log('\nFailed models:');
       for (const f of failed) {
         console.log(`  - ${f.modelId}: ${f.error}`);
       }
     }

     // Exit code based on success rate
     const successRate = successful.length / results.length;
     if (successRate < 0.90) {
       console.log(`\nFailed: Success rate ${(successRate * 100).toFixed(1)}% < 90%`);
       process.exit(1);
     }

     console.log(`\nPassed: ${(successRate * 100).toFixed(1)}% success rate`);
     process.exit(0);
   }
   ```

4. Add npm scripts to package.json:
   - "validate:models": "npx tsx scripts/validate-all-models.ts"
   - "validate:synthetic": "npx tsx scripts/validate-synthetic-models.ts" (keep existing)
   - "test:integration": "vitest run src/__tests__/integration/"

The script:
- Tests all 42 models (not just Synthetic)
- Uses concurrency limit (5) to avoid rate limits
- Reports success/failure per model with timing
- Exits with code 1 if <90% success rate
  </action>
  <verify>
Run `npm run validate:models` with API keys present - should complete and report results.
Check package.json has new scripts.
  </verify>
  <done>
Validation script created and npm scripts added. Running `npm run validate:models` tests all 42 models and reports success rate.
  </done>
</task>

</tasks>

<verification>
1. `npm run test -- --run src/__tests__/integration/` runs integration tests
2. `npm run validate:models` executes full validation against real APIs
3. Test output shows per-model results with JSON structure validation
4. Script exits with code 0 if >90% success, code 1 if <90%
</verification>

<success_criteria>
- Integration test file uses describe.each for all 42 models
- Each test validates JSON structure with Zod schema
- Timeout configured per model type (90s reasoning, 60s standard)
- npm run validate:models runs full validation
- Success rate threshold enforced (90%)
</success_criteria>

<output>
After completion, create `.planning/phases/43-testing-validation/43-02-SUMMARY.md`
</output>
