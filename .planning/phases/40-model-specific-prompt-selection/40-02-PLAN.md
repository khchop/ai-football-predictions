---
phase: 40-model-specific-prompt-selection
plan: 02
type: execute
wave: 2
depends_on: ["40-01"]
files_modified:
  - src/lib/llm/providers/base.ts
  - src/lib/llm/providers/together.ts
  - src/lib/llm/providers/synthetic.ts
autonomous: true

must_haves:
  truths:
    - "GLM models return English responses after receiving English enforcement prompt"
    - "Thinking models (DeepSeek R1, Qwen3-235B-Thinking) return valid JSON without thinking tags"
    - "Kimi K2.5 completes predictions within 60s timeout without timing out"
    - "DeepSeek V3.2 returns valid JSON that passes prediction parser"
    - "Working models (29 Together AI models) maintain success with fallback to base prompt"
  artifacts:
    - path: "src/lib/llm/providers/base.ts"
      provides: "OpenAICompatibleProvider with prompt variant and response handler support"
      contains: "getEnhancedSystemPrompt"
    - path: "src/lib/llm/providers/together.ts"
      provides: "TogetherProvider with PromptConfig constructor parameter"
      contains: "promptConfig"
    - path: "src/lib/llm/providers/synthetic.ts"
      provides: "SyntheticProvider with PromptConfig and configured failing models"
      contains: "promptConfig"
  key_links:
    - from: "src/lib/llm/providers/base.ts"
      to: "src/lib/llm/prompt-variants.ts"
      via: "import getEnhancedSystemPrompt"
      pattern: "import.*getEnhancedSystemPrompt.*from"
    - from: "src/lib/llm/providers/base.ts"
      to: "src/lib/llm/response-handlers.ts"
      via: "import RESPONSE_HANDLERS"
      pattern: "import.*RESPONSE_HANDLERS.*from"
    - from: "src/lib/llm/providers/synthetic.ts"
      to: "GLM46_SynProvider"
      via: "promptConfig with ENGLISH_ENFORCED"
      pattern: "promptVariant.*ENGLISH_ENFORCED"
---

<objective>
Integrate prompt variants and response handlers into provider classes, and configure failing Synthetic models with appropriate settings.

Purpose: Make failing models (GLM, Kimi, DeepSeek V3.2, thinking models) return valid JSON by routing them through specialized prompts and response handlers.

Output: Updated provider classes that use model-specific configurations, with all 6 disabled Synthetic models re-enabled with proper configuration.
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/40-model-specific-prompt-selection/40-CONTEXT.md
@.planning/phases/40-model-specific-prompt-selection/40-RESEARCH.md
@.planning/phases/40-model-specific-prompt-selection/40-01-SUMMARY.md

# Source files to modify
@src/lib/llm/providers/base.ts
@src/lib/llm/providers/together.ts
@src/lib/llm/providers/synthetic.ts

# Dependencies created in Plan 01
@src/lib/llm/prompt-variants.ts
@src/lib/llm/response-handlers.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update base provider to apply prompt variants and response handlers</name>
  <files>src/lib/llm/providers/base.ts</files>
  <action>
Modify OpenAICompatibleProvider.callAPI method to support model-specific configurations:

1. **Add imports** at top of file:
```typescript
import { PromptVariant, PromptConfig, getEnhancedSystemPrompt } from '../prompt-variants';
import { ResponseHandler, RESPONSE_HANDLERS } from '../response-handlers';
```

2. **Add abstract property** to OpenAICompatibleProvider class:
```typescript
// Model-specific configuration (optional, subclasses provide)
protected abstract promptConfig?: PromptConfig;
```

3. **Update callAPI method** to:

   a. **Apply prompt variant** before API call:
   ```typescript
   // Get model-specific variant or default to BASE
   const variant = this.promptConfig?.promptVariant ?? PromptVariant.BASE;
   const enhancedSystemPrompt = getEnhancedSystemPrompt(systemPrompt, variant);
   ```

   b. **Use model-specific timeout**:
   ```typescript
   // Use model-specific timeout if configured, otherwise default
   const modelTimeout = this.promptConfig?.timeoutMs;
   const timeout = modelTimeout ?? (isBatch ? this.batchRequestTimeout : this.requestTimeout);
   ```

   c. **Apply response handler** after receiving content but BEFORE returning:
   ```typescript
   // Apply response handler to process output before parsing
   const handler = this.promptConfig?.responseHandler ?? ResponseHandler.DEFAULT;
   const processedContent = RESPONSE_HANDLERS[handler](content);
   return processedContent;
   ```

4. **Locate where content is extracted** (around line 300-310):
   - After `message?.content` or `message?.reasoning` is extracted
   - Apply the response handler BEFORE the return statement

5. **Ensure backward compatibility**:
   - If promptConfig is undefined, behavior matches current implementation
   - Default variant is BASE (empty addition)
   - Default handler is DEFAULT (pass-through)
   - Default timeout uses existing requestTimeout/batchRequestTimeout

Do NOT change BaseLLMProvider - only OpenAICompatibleProvider needs changes.
  </action>
  <verify>
TypeScript compilation:
```bash
cd /Users/pieterbos/Documents/bettingsoccer && npx tsc --noEmit src/lib/llm/providers/base.ts
```
No errors. Abstract property and imports present.
  </verify>
  <done>
- OpenAICompatibleProvider imports PromptVariant, PromptConfig, getEnhancedSystemPrompt
- OpenAICompatibleProvider imports ResponseHandler, RESPONSE_HANDLERS
- callAPI applies prompt variant to system prompt
- callAPI uses model-specific timeout when configured
- callAPI applies response handler before returning content
- Backward compatible: undefined promptConfig uses defaults
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend TogetherProvider with promptConfig support</name>
  <files>src/lib/llm/providers/together.ts</files>
  <action>
Update TogetherProvider class to accept and store prompt configuration:

1. **Add imports** at top:
```typescript
import { PromptConfig } from '../prompt-variants';
```

2. **Add promptConfig property** to class:
```typescript
public readonly promptConfig: PromptConfig;
```

3. **Update constructor** to accept optional promptConfig parameter:
```typescript
constructor(
  public readonly id: string,
  public readonly name: string,
  public readonly model: string,
  public readonly displayName: string,
  tier: ModelTier,
  pricing: ModelPricing,
  public readonly isPremium: boolean = false,
  promptConfig: PromptConfig = {}  // NEW: optional last parameter with default
) {
  super();
  this.tier = tier;
  this.pricing = pricing;
  this.promptConfig = promptConfig;  // Store the config
}
```

4. **Update DeepSeek R1 provider** (line ~88-96) to use thinking tag configuration:
```typescript
export const DeepSeekR1Provider = new TogetherProvider(
  'deepseek-r1',
  'together',
  'deepseek-ai/DeepSeek-R1',
  'DeepSeek R1 (Reasoning)',
  'premium',
  { promptPer1M: 3.00, completionPer1M: 7.00 },
  true,
  {
    promptVariant: PromptVariant.THINKING_STRIPPED,
    responseHandler: ResponseHandler.STRIP_THINKING_TAGS,
    timeoutMs: 60000,  // 60s for reasoning model
  }
);
```

5. **Add necessary imports** for PromptVariant and ResponseHandler at top:
```typescript
import { PromptConfig, PromptVariant } from '../prompt-variants';
import { ResponseHandler } from '../response-handlers';
```

6. **All other Together AI providers** remain unchanged - they will use default promptConfig (empty object = base prompt, default handler, default timeout).

Note: Only DeepSeek R1 on Together AI needs configuration (thinking model). The 28 other Together AI models work reliably with defaults.
  </action>
  <verify>
TypeScript compilation:
```bash
cd /Users/pieterbos/Documents/bettingsoccer && npx tsc --noEmit src/lib/llm/providers/together.ts
```
No errors. DeepSeekR1Provider has promptConfig.
  </verify>
  <done>
- TogetherProvider constructor accepts optional promptConfig parameter
- promptConfig stored as public readonly property
- DeepSeek R1 configured with THINKING_STRIPPED variant, STRIP_THINKING_TAGS handler, 60s timeout
- All other 28 Together AI providers unchanged (use defaults)
- Backward compatible: existing instantiations without 8th parameter continue to work
  </done>
</task>

<task type="auto">
  <name>Task 3: Configure Synthetic providers and re-enable disabled models</name>
  <files>src/lib/llm/providers/synthetic.ts</files>
  <action>
Update SyntheticProvider and configure all failing models:

1. **Add imports** at top:
```typescript
import { PromptConfig, PromptVariant } from '../prompt-variants';
import { ResponseHandler } from '../response-handlers';
```

2. **Add promptConfig property** to SyntheticProvider class:
```typescript
public readonly promptConfig: PromptConfig;
```

3. **Update SyntheticProvider constructor** to accept promptConfig:
```typescript
constructor(
  public readonly id: string,
  public readonly name: string,
  public readonly model: string,
  public readonly displayName: string,
  tier: ModelTier,
  pricing: ModelPricing,
  public readonly isPremium: boolean = false,
  promptConfig: PromptConfig = {}  // NEW: optional last parameter
) {
  super();
  this.tier = tier;
  this.pricing = pricing;
  this.promptConfig = promptConfig;
}
```

4. **Configure reasoning models** (lines ~84-114):

   **DeepSeek R1 0528** (already works, add config for consistency):
   ```typescript
   export const DeepSeekR1_0528_SynProvider = new SyntheticProvider(
     'deepseek-r1-0528-syn',
     'synthetic',
     'hf:deepseek-ai/DeepSeek-R1-0528',
     'DeepSeek R1 0528 (Synthetic)',
     'premium',
     { promptPer1M: 3.00, completionPer1M: 7.00 },
     true,
     {
       promptVariant: PromptVariant.THINKING_STRIPPED,
       responseHandler: ResponseHandler.STRIP_THINKING_TAGS,
       timeoutMs: 60000,
     }
   );
   ```

   **Kimi K2 Thinking** (already works, add config):
   ```typescript
   {
     promptVariant: PromptVariant.THINKING_STRIPPED,
     responseHandler: ResponseHandler.STRIP_THINKING_TAGS,
     timeoutMs: 60000,
   }
   ```

   **Qwen3 235B Thinking** (currently DISABLED - parse failure):
   ```typescript
   {
     promptVariant: PromptVariant.THINKING_STRIPPED,
     responseHandler: ResponseHandler.STRIP_THINKING_TAGS,
     timeoutMs: 90000,  // Extra time for large thinking model
   }
   ```

5. **Configure DeepSeek V3 family** (lines ~120-152):

   **DeepSeek V3.2** (currently DISABLED - returns natural language):
   ```typescript
   {
     promptVariant: PromptVariant.JSON_STRICT,
     responseHandler: ResponseHandler.EXTRACT_JSON,
     timeoutMs: 45000,
   }
   ```

6. **Configure Kimi K2.5** (line ~187-195, currently DISABLED - timeout):
   ```typescript
   {
     promptVariant: PromptVariant.BASE,
     responseHandler: ResponseHandler.DEFAULT,
     timeoutMs: 60000,  // Increased from 30s default
   }
   ```

7. **Configure GLM models** (lines ~203-222, currently DISABLED):

   **GLM 4.6** (timeout + may return Chinese):
   ```typescript
   {
     promptVariant: PromptVariant.ENGLISH_ENFORCED,
     responseHandler: ResponseHandler.DEFAULT,
     timeoutMs: 60000,
   }
   ```

   **GLM 4.7** (API bug noted - try English enforcement):
   ```typescript
   {
     promptVariant: PromptVariant.ENGLISH_ENFORCED,
     responseHandler: ResponseHandler.EXTRACT_JSON,
     timeoutMs: 60000,
   }
   ```

8. **Configure GPT-OSS 120B** (line ~246-254, currently DISABLED - invalid response):
   ```typescript
   {
     promptVariant: PromptVariant.JSON_STRICT,
     responseHandler: ResponseHandler.EXTRACT_JSON,
     timeoutMs: 45000,
   }
   ```

9. **MOVE DISABLED MODELS BACK TO SYNTHETIC_PROVIDERS ARRAY**:

   Update line ~273-288 to include ALL 13 models:
   ```typescript
   export const SYNTHETIC_PROVIDERS = [
     // Reasoning models (3) - Premium
     DeepSeekR1_0528_SynProvider,        // 1  - $3.00/$7.00 (premium, reasoning)
     KimiK2Thinking_SynProvider,         // 2  - $2.00/$6.00 (premium, reasoning)
     Qwen3_235BThinking_SynProvider,     // 3  - $2.50/$6.00 (premium, reasoning) - RE-ENABLED

     // DeepSeek family (3) - Budget
     DeepSeekV3_0324_SynProvider,        // 4  - $0.60/$1.25
     DeepSeekV31_Terminus_SynProvider,   // 5  - $0.70/$1.40
     DeepSeekV32_SynProvider,            // 6  - $0.65/$1.30 - RE-ENABLED

     // MiniMax (2) - Budget
     MiniMaxM2_SynProvider,              // 7  - $0.50/$1.00
     MiniMaxM21_SynProvider,             // 8  - $0.55/$1.10

     // Moonshot (1) - Budget
     KimiK25_SynProvider,                // 9  - $1.00/$3.00 - RE-ENABLED

     // GLM (2) - Budget
     GLM46_SynProvider,                  // 10 - $0.40/$0.80 - RE-ENABLED
     GLM47_SynProvider,                  // 11 - $0.45/$0.90 - RE-ENABLED

     // Qwen Coder (1) - Premium
     Qwen3Coder480B_SynProvider,         // 12 - $3.00/$6.00 (premium)

     // OpenAI OSS (1) - Budget
     GPTOSS120B_SynProvider,             // 13 - $1.20/$2.40 - RE-ENABLED
   ];
   ```

10. **Update the DISABLED MODELS comment block** (lines ~257-267):
    - Remove the disabled models list
    - Add a note that all models are now configured with appropriate prompt variants

11. **Update the Summary comment** (lines ~290-298):
    - Change "7 models ACTIVE" to "13 models ACTIVE"
    - Change "6 models DISABLED" to "0 models DISABLED"
    - Add note about prompt variants being used for reliability
  </action>
  <verify>
TypeScript compilation:
```bash
cd /Users/pieterbos/Documents/bettingsoccer && npx tsc --noEmit src/lib/llm/providers/synthetic.ts
```
No errors.

Verify all 13 models in array:
```bash
cd /Users/pieterbos/Documents/bettingsoccer && grep -c "SynProvider," src/lib/llm/providers/synthetic.ts
```
Should show 13.

Verify prompt configs present:
```bash
cd /Users/pieterbos/Documents/bettingsoccer && grep -c "promptVariant:" src/lib/llm/providers/synthetic.ts
```
Should show 8+ (reasoning models + failing models).
  </verify>
  <done>
- SyntheticProvider constructor accepts optional promptConfig parameter
- DeepSeek R1 0528 configured with THINKING_STRIPPED, 60s timeout
- Kimi K2 Thinking configured with THINKING_STRIPPED, 60s timeout
- Qwen3 235B Thinking RE-ENABLED with THINKING_STRIPPED, 90s timeout
- DeepSeek V3.2 RE-ENABLED with JSON_STRICT, EXTRACT_JSON handler, 45s timeout
- Kimi K2.5 RE-ENABLED with 60s timeout
- GLM 4.6 RE-ENABLED with ENGLISH_ENFORCED, 60s timeout
- GLM 4.7 RE-ENABLED with ENGLISH_ENFORCED, EXTRACT_JSON handler, 60s timeout
- GPT-OSS 120B RE-ENABLED with JSON_STRICT, EXTRACT_JSON handler, 45s timeout
- SYNTHETIC_PROVIDERS array contains all 13 models
- All requirements PRMT-01 through PRMT-06 satisfied
- All requirements TIME-01 through TIME-04 satisfied
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Full TypeScript compilation:**
```bash
cd /Users/pieterbos/Documents/bettingsoccer && npx tsc --noEmit
```
No errors across entire codebase.

2. **Verify provider count:**
```bash
cd /Users/pieterbos/Documents/bettingsoccer && node -e "
const { TOGETHER_PROVIDERS } = require('./src/lib/llm/providers/together');
const { SYNTHETIC_PROVIDERS } = require('./src/lib/llm/providers/synthetic');
console.log('Together providers:', TOGETHER_PROVIDERS.length);
console.log('Synthetic providers:', SYNTHETIC_PROVIDERS.length);
console.log('Total:', TOGETHER_PROVIDERS.length + SYNTHETIC_PROVIDERS.length);
"
```
Expected: Together: 29, Synthetic: 13, Total: 42

3. **Verify prompt configs:**
```bash
cd /Users/pieterbos/Documents/bettingsoccer && node -e "
const { DeepSeekR1Provider } = require('./src/lib/llm/providers/together');
const { GLM46_SynProvider, Qwen3_235BThinking_SynProvider } = require('./src/lib/llm/providers/synthetic');
console.log('DeepSeek R1 config:', DeepSeekR1Provider.promptConfig);
console.log('GLM 4.6 config:', GLM46_SynProvider.promptConfig);
console.log('Qwen3 Thinking config:', Qwen3_235BThinking_SynProvider.promptConfig);
"
```
Each should show promptVariant, responseHandler, timeoutMs.

4. **Verify existing tests pass:**
```bash
cd /Users/pieterbos/Documents/bettingsoccer && npm test -- --passWithNoTests
```
</verification>

<success_criteria>
**Requirements satisfied:**
- PRMT-01: Model-specific prompt templates exist for failing models (GLM, Kimi, thinking models)
- PRMT-02: GLM models receive English enforcement prompt
- PRMT-03: Thinking models receive tag suppression prompt (via THINKING_STRIPPED variant)
- PRMT-04: DeepSeek R1 uses appropriate prompt for JSON output
- PRMT-05: Prompt selector falls back to base prompt for unmapped models
- PRMT-06: Models returning natural language receive JSON emphasis prompt (JSON_STRICT)
- JSON-01: Models that fail JSON parsing receive structured output enforcement
- JSON-02: qwen3-235b-thinking returns valid JSON after prompt adjustment
- JSON-03: deepseek-v3.2 returns valid JSON after prompt adjustment
- TIME-01: Model-specific timeout configuration exists (per-model timeoutMs)
- TIME-02: Thinking models use 60s timeout (some 90s for very large)
- TIME-03: Kimi K2.5 uses 60s timeout
- TIME-04: GLM models use 60s timeout

**Measurable outcomes:**
- 42 total providers (29 Together + 13 Synthetic)
- 6 previously disabled models now re-enabled with configurations
- All models compile without errors
- Working models unchanged (default configs)
</success_criteria>

<output>
After completion, create `.planning/phases/40-model-specific-prompt-selection/40-02-SUMMARY.md`
</output>
