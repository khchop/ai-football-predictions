---
phase: 01-critical-stability
plan: 04
type: execute
wave: 1
depends_on: []
files_modified: [src/lib/utils/retry-config.ts, src/lib/utils/api-client.ts, src/lib/queue/workers/model-recovery.worker.ts, src/lib/db/queries.ts]
autonomous: true
must_haves:
  truths:
    - "Rate limit errors trigger 60s backoff without counting toward model disable"
    - "Timeout errors trigger linear backoff (5s, 10s, 15s...) without counting toward model disable"
    - "Model-specific errors (parse failures, 4xx) count toward 5-failure disable threshold"
    - "Auto-disabled models recover after 1h cooldown with partial reset (2 failures)"
    - "Recovery worker runs every 30 minutes checking for cooldown expiration"
  artifacts:
    - path: "src/lib/utils/retry-config.ts"
      provides: "Error-type-aware backoff configuration"
      contains: "calculateBackoffDelay|ErrorType|RetryConfig"
    - path: "src/lib/utils/api-client.ts"
      provides: "fetchWithRetry with circuit breaker"
      contains: "fetchWithRetry|fetchWithTimeout"
    - path: "src/lib/queue/workers/model-recovery.worker.ts"
      provides: "Automated model recovery worker"
      contains: "recoverDisabledModels|recordModelFailure|recordModelSuccess"
    - path: "src/lib/db/queries.ts"
      provides: "Database queries for model health tracking"
      exports: ["updateModelHealth", "getAutoDisabledModels", "updateModelRecovery"]
  key_links:
    - from: "src/lib/utils/api-client.ts"
      to: "src/lib/utils/retry-config.ts"
      via: "calculateBackoffDelay import"
      pattern: "import.*calculateBackoffDelay.*from.*retry-config"
    - from: "src/lib/queue/workers/model-recovery.worker.ts"
      to: "src/lib/db/queries.ts"
      via: "getAutoDisabledModels, updateModelRecovery calls"
      pattern: "getAutoDisabledModels\(|updateModelRecovery\("
---

<objective>
Implement error-type-aware timeout handling and model failure classification with automated cooldown recovery.

Purpose: Distinguish transient API errors (rate limits, timeouts) from model-specific failures (parse errors, 4xx) to prevent bad models from being disabled due to service outages, and automate recovery of auto-disabled models after cooldown.
Output: Robust timeout handling, error classification, and automated model recovery system.
</objective>

<execution_context>
@/Users/pieterbos/.config/opencode/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Research: Timeout handling and model failure classification
From .planning/phases/01-critical-stability/01-RESEARCH.md:
- Current issue: All errors count toward disable threshold, transient errors disable good models, no automated recovery
- Pattern: Error classification (transient: rate-limit/timeout/5xx, model-specific: parse/4xx), separate counters for each type
- Backoff: Rate limit (60s fixed), timeout (linear 5s,10s,15s...), parse (exponential 5s,10s,20s...)
- Disable threshold: 5 consecutive MODEL-SPECIFIC failures (transient don't count)
- Recovery: 1 hour cooldown, partial reset to 2 failures (require 3 more before re-disable)
- Automated recovery: Worker runs every 30 minutes, queries disabled models, checks cooldown

# Context decisions:
- Error classification: Treat 5xx as transient (don't count toward disable), treat parse errors and 4xx as model-specific
- Recovery failure count reset: Use partial reset (2 failures) requiring 3 more failures before re-disable
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create error-type-aware backoff configuration</name>
  <files>src/lib/utils/retry-config.ts</files>
  <action>
Create src/lib/utils/retry-config.ts with backoff strategies:

```typescript
export enum ErrorType {
  RATE_LIMIT = 'rate-limit',
  TIMEOUT = 'timeout',
  SERVER_ERROR = 'server-error',
  NETWORK_ERROR = 'network-error',
  PARSE_ERROR = 'parse-error',
  CLIENT_ERROR = 'client-error',
  UNKNOWN = 'unknown',
}

export interface RetryConfig {
  maxRetries: number;
  baseDelayMs: number;
  retryableStatusCodes: number[];
  enableJitter: boolean;
}

export const DEFAULT_RETRY_CONFIG: RetryConfig = {
  maxRetries: 5,
  baseDelayMs: 1000,
  retryableStatusCodes: [408, 429, 500, 502, 503, 504],
  enableJitter: true,
};

/**
 * Classify error based on HTTP status or error message
 */
export function classifyErrorType(error: unknown): ErrorType {
  const errorMsg = error instanceof Error ? error.message : String(error);

  // HTTP status codes
  if (errorMsg.includes('429')) return ErrorType.RATE_LIMIT;
  if (errorMsg.includes('500') || errorMsg.includes('502') || errorMsg.includes('503') || errorMsg.includes('504')) {
    return ErrorType.SERVER_ERROR;
  }
  if (errorMsg.includes('400-')) return ErrorType.CLIENT_ERROR;

  // Error messages
  if (errorMsg.includes('timeout') || errorMsg.includes('ETIMEDOUT')) return ErrorType.TIMEOUT;
  if (errorMsg.includes('ECONNREFUSED') || errorMsg.includes('ENOTFOUND') || errorMsg.includes('ECONNRESET')) {
    return ErrorType.NETWORK_ERROR;
  }
  if (errorMsg.includes('parse') || errorMsg.includes('JSON') || errorMsg.includes('unexpected')) {
    return ErrorType.PARSE_ERROR;
  }

  return ErrorType.UNKNOWN;
}

/**
 * Calculate backoff delay based on error type and attempt number
 */
export function calculateBackoffDelay(
  attempt: number,
  errorType: ErrorType,
  config: RetryConfig = DEFAULT_RETRY_CONFIG
): number {
  const baseDelay = config.baseDelayMs;

  switch (errorType) {
    case ErrorType.RATE_LIMIT:
      // Rate limits need longer, consistent backoff (no jitter)
      return 60000; // 60s fixed

    case ErrorType.TIMEOUT:
      // Timeouts benefit from linear backoff (faster recovery)
      const linearDelay = attempt * 5000;
      return Math.min(linearDelay, 30000); // 5s, 10s, 15s... max 30s

    case ErrorType.PARSE_ERROR:
      // Parse errors retry quickly with exponential (transient LLM formatting issues)
      const parseDelay = 5000 * Math.pow(2, attempt - 1);
      return Math.min(parseDelay, 20000); // 5s, 10s, 20s...

    case ErrorType.SERVER_ERROR:
    case ErrorType.NETWORK_ERROR:
      // Server and network errors use exponential backoff with jitter
      const exponentialDelay = baseDelay * Math.pow(2, attempt);
      if (config.enableJitter) {
        const jitter = Math.random() * 0.3; // 30% jitter to prevent thundering herd
        return Math.min(exponentialDelay * (1 + jitter), 60000);
      }
      return Math.min(exponentialDelay, 60000);

    default:
      // Unknown errors: exponential backoff
      const unknownDelay = baseDelay * Math.pow(2, attempt);
      return Math.min(unknownDelay, 60000);
  }
}

/**
 * Determine if an error should be counted toward model disable threshold
 * Transient errors (rate limits, timeouts, server errors) should NOT count toward disable
 */
export function isModelSpecificFailure(errorType: ErrorType): boolean {
  return (
    errorType === ErrorType.PARSE_ERROR ||
    errorType === ErrorType.CLIENT_ERROR
  );
}
```

Export functions for use in api-client and workers.
</action>
  <verify>grep -E "enum ErrorType|function classifyErrorType|function calculateBackoffDelay|function isModelSpecificFailure" src/lib/utils/retry-config.ts</verify>
  <done>Error-type-aware backoff created: 6 error types, classify function, calculate delay with jitter support, model-specific failure detection</done>
</task>

<task type="auto">
  <name>Task 2: Implement fetchWithRetry with timeout and circuit breaker</name>
  <files>src/lib/utils/api-client.ts</files>
  <action>
Create or update src/lib/utils/api-client.ts:

```typescript
import { logger } from '../logger';
import {
  classifyErrorType,
  calculateBackoffDelay,
  RetryConfig,
  DEFAULT_RETRY_CONFIG,
  isModelSpecificFailure,
} from './retry-config';

// Circuit breaker state tracking
const circuitBreakerState = new Map<string, {
  state: 'closed' | 'open' | 'half-open';
  lastFailureTime: number;
  failureCount: number;
}>();

export type ServiceName = 'API-Football' | 'Together-AI' | 'Other';

/**
 * Fetch with timeout
 */
async function fetchWithTimeout(
  url: string,
  options: RequestInit = {},
  timeoutMs: number = 30000
): Promise<Response> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), timeoutMs);

  try {
    const response = await fetch(url, { ...options, signal: controller.signal });
    clearTimeout(timeoutId);
    return response;
  } catch (error) {
    clearTimeout(timeoutId);
    if (error instanceof Error && error.name === 'AbortError') {
      throw new Error(`Request timeout after ${timeoutMs}ms`);
    }
    throw error;
  }
}

/**
 * Check circuit breaker state
 */
function checkCircuitBreaker(serviceName: ServiceName): boolean {
  const state = circuitBreakerState.get(serviceName);

  if (!state || state.state !== 'open') {
    return false; // Not blocked
  }

  // Check if cooldown period expired (5 minutes)
  const COOLDOWN_MS = 5 * 60 * 1000;
  const timeSinceFailure = Date.now() - state.lastFailureTime;

  if (timeSinceFailure > COOLDOWN_MS) {
    // Move to half-open state
    state.state = 'half-open';
    logger.info({ serviceName }, 'Circuit breaker moved to half-open state');
    return false;
  }

  return true; // Blocked by circuit breaker
}

/**
 * Record circuit breaker failure
 */
function recordCircuitBreakerFailure(serviceName: ServiceName) {
  const state = circuitBreakerState.get(serviceName) || {
    state: 'closed' as const,
    lastFailureTime: 0,
    failureCount: 0,
  };

  state.lastFailureTime = Date.now();
  state.failureCount++;

  // Open circuit after 5 consecutive failures
  if (state.failureCount >= 5) {
    state.state = 'open';
    logger.error({ serviceName, failureCount: state.failureCount }, 'Circuit breaker opened');
  }

  circuitBreakerState.set(serviceName, state);
}

/**
 * Record circuit breaker success
 */
function recordCircuitBreakerSuccess(serviceName: ServiceName) {
  const state = circuitBreakerState.get(serviceName);

  if (state) {
    if (state.state === 'half-open') {
      // Move back to closed after successful request
      state.state = 'closed';
      state.failureCount = 0;
      logger.info({ serviceName }, 'Circuit breaker moved to closed state');
    } else {
      state.failureCount = 0;
    }
    circuitBreakerState.set(serviceName, state);
  }
}

/**
 * Fetch with retry, timeout, and circuit breaker
 */
export async function fetchWithRetry(
  url: string,
  options: RequestInit = {},
  timeoutMs: number = 30000,
  retryConfig: RetryConfig = DEFAULT_RETRY_CONFIG,
  serviceName?: ServiceName
): Promise<Response> {
  // Check circuit breaker
  if (serviceName && checkCircuitBreaker(serviceName)) {
    throw new Error(`Circuit breaker open for ${serviceName}`);
  }

  for (let attempt = 0; attempt <= retryConfig.maxRetries; attempt++) {
    try {
      const response = await fetchWithTimeout(url, options, timeoutMs);

      if (response.ok) {
        // Record success for circuit breaker
        if (serviceName) {
          recordCircuitBreakerSuccess(serviceName);
        }
        return response;
      }

      // Classify error from HTTP status
      const errorType = classifyErrorType(new Error(`HTTP ${response.status}`));

      // Check if status code is retryable
      if (!retryConfig.retryableStatusCodes.includes(response.status)) {
        logger.error({
          url,
          status: response.status,
          attempt,
        }, 'Non-retryable status code');
        throw new Error(`Non-retryable status: ${response.status}`);
      }

      // Calculate backoff for retry
      const delay = calculateBackoffDelay(attempt, errorType, retryConfig);
      logger.warn({
        url,
        status: response.status,
        attempt,
        errorType,
        delayMs: delay,
      }, `Retrying after ${delay}ms`);

      await new Promise(resolve => setTimeout(resolve, delay));

    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);

      // Classify error
      let errorType: ErrorType;
      if (errorMsg.includes('timeout') || errorMsg.includes('AbortError')) {
        errorType = ErrorType.TIMEOUT;
      } else if (errorMsg.includes('Circuit breaker')) {
        throw error; // Don't retry circuit breaker errors
      } else {
        errorType = classifyErrorType(error);
      }

      // Check if we've exhausted retries
      if (attempt >= retryConfig.maxRetries) {
        if (serviceName) {
          recordCircuitBreakerFailure(serviceName);
        }
        logger.error({
          url,
          error: errorMsg,
          attempt,
          errorType,
        }, 'Max retries exceeded');
        throw error;
      }

      // Calculate backoff for retry
      const delay = calculateBackoffDelay(attempt, errorType, retryConfig);
      logger.warn({
        url,
        error: errorMsg,
        attempt,
        errorType,
        delayMs: delay,
      }, `Retrying after ${delay}ms`);

      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }

  throw new Error('Max retries exceeded');
}
```

Export fetchWithRetry and related circuit breaker functions.
</action>
  <verify>grep -E "function fetchWithRetry|function checkCircuitBreaker|function recordCircuitBreaker" src/lib/utils/api-client.ts</verify>
  <done>fetchWithRetry implemented with: timeout support, error-type-aware backoff, circuit breaker (closed/open/half-open), service name tracking</done>
</task>

<task type="auto">
  <name>Task 3: Add model health tracking queries</name>
  <files>src/lib/db/queries.ts</files>
  <action>
Read src/lib/db/queries.ts. Add model health tracking functions:

```typescript
import { db } from './index';
import { models } from './schema';
import { eq, sql, and, lt, isNotNull } from 'drizzle-orm';

const DISABLE_THRESHOLD = 5; // Consecutive failures before auto-disable
const COOLDOWN_MS = 60 * 60 * 1000; // 1 hour cooldown

/**
 * Record model failure with error classification
 * Only counts toward disable threshold for model-specific errors
 */
export async function recordModelFailure(
  modelId: string,
  error: string,
  errorType: string
): Promise<{ autoDisabled: boolean; consecutiveFailures: number }> {
  const isModelSpecific = errorType === 'parse-error' || errorType === 'client-error';

  // Only increment consecutive failures for model-specific errors
  const incrementExpr = isModelSpecific
    ? sql`COALESCE(${models.consecutiveFailures}, 0) + 1`
    : models.consecutiveFailures;

  const result = await db
    .update(models)
    .set({
      consecutiveFailures: incrementExpr,
      lastFailureAt: new Date().toISOString(),
      failureReason: error.substring(0, 500),
      // Auto-disable only for model-specific failures at threshold
      ...(isModelSpecific && {
        autoDisabled: sql`CASE WHEN COALESCE(${models.consecutiveFailures}, 0) + 1 >= ${DISABLE_THRESHOLD} THEN TRUE ELSE ${models.autoDisabled} END`,
      }),
    })
    .where(eq(models.id, modelId))
    .returning({
      autoDisabled: models.autoDisabled,
      consecutiveFailures: models.consecutiveFailures,
    });

  const updated = result[0];
  if (updated?.autoDisabled && !isModelSpecific) { // Prevent spurious logs for non-model-specific
    logger.warn({ modelId, threshold: DISABLE_THRESHOLD }, 'Model auto-disabled');
  }

  return {
    autoDisabled: updated?.autoDisabled || false,
    consecutiveFailures: updated?.consecutiveFailures || 0,
  };
}

/**
 * Record model success (reset counters)
 */
export async function recordModelSuccess(modelId: string): Promise<void> {
  await db
    .update(models)
    .set({
      consecutiveFailures: 0,
      lastSuccessAt: new Date().toISOString(),
      failureReason: null,
      autoDisabled: false, // Re-enable on success
    })
    .where(eq(models.id, modelId));
}

/**
 * Get all auto-disabled models
 */
export async function getAutoDisabledModels(): Promise<Array<{
  id: string;
  name: string;
  consecutiveFailures: number;
  lastFailureAt: string;
  failureReason: string | null;
}>> {
  return await db
    .select({
      id: models.id,
      name: models.name,
      consecutiveFailures: models.consecutiveFailures,
      lastFailureAt: models.lastFailureAt,
      failureReason: models.failureReason,
    })
    .from(models)
    .where(and(
      eq(models.autoDisabled, true),
      isNotNull(models.lastFailureAt)
    ));
}

/**
 * Recover auto-disabled models after cooldown
 * Partial reset: consecutiveFailures = 2 (require 3 more failures before re-disable)
 */
export async function recoverDisabledModels(): Promise<number> {
  const disabledModels = await getAutoDisabledModels();
  let recoveredCount = 0;

  for (const model of disabledModels) {
    const lastFailure = model.lastFailureAt ? new Date(model.lastFailureAt).getTime() : 0;
    const timeSinceFailure = Date.now() - lastFailure;

    if (timeSinceFailure >= COOLDOWN_MS) {
      // Reset to partial count (2) - not fully trusted yet
      await db
        .update(models)
        .set({
          autoDisabled: false,
          consecutiveFailures: 2,
          failureReason: 'Recovered after cooldown',
        })
        .where(eq(models.id, model.id));

      logger.info({
        modelId: model.id,
        modelName: model.name,
        consecutiveFailures: 2,
        cooldownHours: COOLDOWN_MS / (60 * 60 * 1000),
      }, 'Model re-enabled after cooldown');

      recoveredCount++;
    }
  }

  return recoveredCount;
}
```

Add to existing queries file or create if doesn't exist.
</action>
  <verify>grep -E "async function recordModelFailure|async function recordModelSuccess|async function recoverDisabledModels" src/lib/db/queries.ts</verify>
  <done>Model health queries created: recordModelFailure (with error classification), recordModelSuccess (reset counters), recoverDisabledModels (partial reset to 2)</done>
</task>

<task type="auto">
  <name>Task 4: Create model recovery worker</name>
  <files>src/lib/queue/workers/model-recovery.worker.ts</files>
  <action>
Create src/lib/queue/workers/model-recovery.worker.ts:

```typescript
import { Worker } from 'bullmq';
import { getQueueConnection } from '..';
import { QUEUE_NAMES } from '../types';
import { recoverDisabledModels } from '../../db/queries';
import { logger } from '../../logger';

/**
 * Model recovery worker - runs every 30 minutes to check for cooldown expiration
 * Re-enables auto-disabled models after 1-hour cooldown with partial failure count reset (2)
 */
export function createModelRecoveryWorker() {
  const worker = new Worker(
    QUEUE_NAMES.MODEL_RECOVERY,
    async (job) => {
      const log = logger.child({
        module: 'model-recovery-worker',
        jobId: job.id,
      });

      log.info('Starting model recovery check');

      try {
        const recoveredCount = await recoverDisabledModels();

        log.info({ recoveredCount }, 'Model recovery check completed');
        return { recoveredCount };

      } catch (error) {
        log.error({
          error: error instanceof Error ? error.message : String(error),
        }, 'Model recovery check failed');
        throw error;
      }
    },
    {
      connection: getQueueConnection(),
      concurrency: 1, // One recovery job at a time
    }
  );

  // Worker event handlers
  worker.on('completed', (job, result) => {
    logger.info({
      jobId: job.id,
      recoveredCount: result?.recoveredCount || 0,
    }, 'Model recovery job completed');
  });

  worker.on('failed', (job, err) => {
    logger.error({
      jobId: job?.id,
      error: err.message,
    }, 'Model recovery job failed');
  });

  worker.on('error', (err) => {
    logger.error({ error: err.message }, 'Model recovery worker error');
  });

  return worker;
}

/**
 * Schedule repeatable recovery job
 */
export async function scheduleRecoveryJob(): Promise<void> {
  // This should be called during application startup
  // The job should be scheduled via the queue system
  logger.info('Model recovery job scheduled to run every 30 minutes');
}
```

Create worker file with repeatable job scheduling.
</action>
  <verify>grep -E "function createModelRecoveryWorker|function scheduleRecoveryJob" src/lib/queue/workers/model-recovery.worker.ts</verify>
  <done>Model recovery worker created: triggers every 30 minutes, calls recoverDisabledModels(), logs recovery count</done>
</task>

<task type="auto">
  <name>Task 5: Update prediction worker to use error classification</name>
  <files>src/lib/queue/workers/predictions.worker.ts</files>
  <action>
Read src/lib/queue/workers/predictions.worker.ts. Update model error handling to classify errors:

```typescript
import { classifyErrorType, isModelSpecificFailure, ErrorType } from '../../utils/retry-config';
import { recordModelFailure, recordModelSuccess } from '../../db/queries';

// Update model processing loop in processPredictionJob:

for (const provider of providers) {
  try {
    const rawResponse = await provider.callAPI(BATCH_SYSTEM_PROMPT, prompt);

    // Validate response before parsing
    if (!rawResponse || typeof rawResponse !== 'string') {
      const errorType = ErrorType.PARSE_ERROR; // Empty response treated as parse error
      log.warn({ modelId: provider.id, errorType }, 'Provider returned null/invalid response');

      await recordModelFailure(provider.id, 'empty_response', errorType);
      failedModels.push({ modelId: provider.id, error: 'empty_response' });
      continue;
    }

    const parsed = parseBatchPredictionResponse(rawResponse, [job.data.matchId]);

    if (!parsed.success) {
      const errorType = ErrorType.PARSE_ERROR;
      log.warn({
        modelId: provider.id,
        errorType,
        preview: rawResponse.slice(0, 500),
        error: parsed.error,
      }, 'Parse failed');

      await recordModelFailure(provider.id, parsed.error || 'Parse failed', errorType);
      failedModels.push({ modelId: provider.id, error: parsed.error || 'Parse failed' });
      continue;
    }

    predictions.push({
      matchId: job.data.matchId,
      modelId: provider.id,
      predictedHome: parsed.predictions[0].homeScore,
      predictedAway: parsed.predictions[0].awayScore,
      createdAt: new Date(),
    });
    successfulModels.push(provider.id);

  } catch (modelError) {
    const errorMsg = modelError instanceof Error ? modelError.message : String(modelError);
    const errorType = classifyErrorType(modelError);

    log.warn({
      modelId: provider.id,
      errorType,
      error: errorMsg,
    }, 'Model prediction failed');

    await recordModelFailure(provider.id, errorMsg, errorType);
    failedModels.push({ modelId: provider.id, error: errorMsg });

    // Log if error is model-specific (counts toward disable)
    if (isModelSpecificFailure(errorType)) {
      log.warn({ modelId: provider.id, errorType, errorMsg }, 'Model-specific failure - counts toward disable threshold');
    }
  }
}
```

Update model error handling to call recordModelFailure with error type classification.
</action>
  <verify>grep -E "classifyErrorType|recordModelFailure|isModelSpecificFailure" src/lib/queue/workers/predictions.worker.ts</verify>
  <done>Prediction worker updated to classify errors, recordModelFailure with errorType, log model-specific failures for disable tracking</done>
</task>

</tasks>

<verification>
1. Rate limit backoff: Simulate 429 error, verify backoff is 60s (not exponential)
2. Timeout backoff: Simulate timeout error, verify backoff is linear (5s, 10s, 15s...)
3. Parse error backoff: Simulate parse error, verify backoff is exponential (5s, 10s, 20s...)
4. Model-specific counting: Force parse errors 5 times, verify autoDisabled=true after 5th
5. Transient error not counting: Force rate limit errors 10 times, verify consecutiveFailures does NOT increment
6. Recovery worker: Manually disable model, advance lastFailureAt by 61 minutes, run recovery job, verify recovered and consecutiveFailures=2
7. Circuit breaker: Force 5 consecutive 500 errors, verify circuit opens and subsequent requests fail immediately
</verification>

<success_criteria>
- Rate limit errors trigger 60s backoff without counting toward model disable threshold
- Timeout errors trigger linear backoff (5s, 10s, 15s...) without counting toward model disable
- Model-specific errors (parse failures, 4xx) count toward 5-failure disable threshold
- Auto-disabled models only trigger on 5+ consecutive MODEL-SPECIFIC failures
- Models auto-disabled after threshold and logged with warning
- Recovery worker runs every 30 minutes checking for cooldown expiration
- Auto-disabled models recover after 1h cooldown with partial reset (consecutiveFailures=2)
- Circuit breaker opens after 5 consecutive failures, moves to half-open after cooldown
- Circuit breaker state tracking prevents API flood after service recovery
</success_criteria>

<output>
After completion, create `.planning/phases/01-critical-stability/01-04-SUMMARY.md`

Use summary template with:
- Error-type-aware backoff implemented (rate-limit: 60s, timeout: linear, parse: exponential)
- Model failure classification working (transient vs model-specific)
- Auto-disable threshold working (5 model-specific failures)
- Recovery worker deployed (runs every 30min, partial reset to 2)
- Circuit breaker implemented (closed/open/half-open states)
- Any codebase patterns discovered
- Issues encountered and resolutions
</output>
