---
phase: 01-critical-stability
plan: 02
type: execute
wave: 1
depends_on: []
files_modified: [src/lib/queue/workers/predictions.worker.ts, src/lib/queue/types.ts, src/lib/logger/index.ts]
autonomous: true
must_haves:
  truths:
    - "Null/malformed API responses don't crash workers"
    - "Workers classify errors as retryable vs unrecoverable"
    - "Individual model errors don't fail entire batch (isolation)"
    - "Workers continue processing when one model fails"
    - "Job lifecycle logged (start, success, failure, retry)"
  artifacts:
    - path: "src/lib/queue/workers/predictions.worker.ts"
      provides: "Defensive prediction worker with error classification"
      contains: "Worker(", "try {", "catch", "retryable|unrecoverable"
    - path: "src/lib/queue/types.ts"
      provides: "Job payload and result type definitions"
      exports: ["PredictionJob", "JobResult", "JobStatus"]
  key_links:
    - from: "src/lib/queue/workers/predictions.worker.ts"
      to: "src/lib/queue/types.ts"
      via: "type imports"
      pattern: "import.*from.*types"
    - from: "src/lib/queue/workers/predictions.worker.ts"
      to: "logger"
      via: "child logger creation"
      pattern: "logger\.child\({ jobId"
---

<objective>
Add defensive error handling to prediction queue worker with null checks, error classification, and isolated failures.

Purpose: Prevent worker crashes from null/malformed API data, implement retry logic for transient errors, and ensure individual model failures don't stop batch processing.
Output: Robust worker with comprehensive error handling and isolation.
</objective>

<execution_context>
@/Users/pieterbos/.config/opencode/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Research: Defensive worker error handling
From .planning/phases/01-critical-stability/01-RESEARCH.md:
- Current issue: Workers crash on null API responses, no error classification
- Pattern: Worker-level try-catch with error classification (retryable: timeout/ECONNREFUSED, unrecoverable: match started/cancelled)
- Isolation: Wrap each model processing in try-catch, return partial results
- BullMQ backoffStrategy hook for error-type-aware retries
- Verbose logging: Log all job starts/completions with child logger per job

# Context decisions:
- Retry count: 5 retries before giving up
- UnrecoverableError triggers: Match started, cancelled, postponed
- Worker isolation: Full isolation - each worker independent
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define job types and result interfaces</name>
  <files>src/lib/queue/types.ts</files>
  <action>
Read src/lib/queue/types.ts. Add or update job type definitions:

```typescript
export interface PredictionJob {
  matchId: string;
  scheduledAt: string;
}

export interface PredictionJobData extends PredictionJob {
  retryCount?: number;
}

export interface JobResult {
  success: boolean;
  predictionCount?: number;
  failedCount?: number;
  skipped?: boolean;
  reason?: string;
  error?: string;
}

export enum JobStatus {
  PENDING = 'pending',
  PROCESSING = 'processing',
  COMPLETED = 'completed',
  FAILED = 'failed',
  SKIPPED = 'skipped',
}

// Worker configuration
export interface WorkerConfig {
  concurrency: number;
  maxRetries: number;
}
```

If file doesn't exist, create it with these types.
</action>
  <verify>grep -E "interface PredictionJob|interface JobResult|enum JobStatus" src/lib/queue/types.ts</verify>
  <done>Job types defined with PredictionJob, JobResult, JobStatus, WorkerConfig</done>
</task>

<task type="auto">
  <name>Task 2: Add error classification helper</name>
  <files>src/lib/queue/workers/predictions.worker.ts</files>
  <action>
Read existing src/lib/queue/workers/predictions.worker.ts. Add error classification function:

```typescript
type ErrorType = 'retryable' | 'unrecoverable' | 'unknown';

function classifyError(error: unknown): ErrorType {
  const errorMsg = error instanceof Error ? error.message : String(error);

  // Retryable: network errors, timeouts, connection issues
  if (errorMsg.includes('timeout') ||
      errorMsg.includes('ECONNREFUSED') ||
      errorMsg.includes('ETIMEDOUT') ||
      errorMsg.includes('ENOTFOUND') ||
      errorMsg.includes('ECONNRESET')) {
    return 'retryable';
  }

  // Unrecoverable: business logic issues that won't fix on retry
  if (errorMsg.includes('match started') ||
      errorMsg.includes('cancelled') ||
      errorMsg.includes('postponed') ||
      errorMsg.includes('kickoff passed')) {
    return 'unrecoverable';
  }

  // Unknown: retry but with backoff
  return 'unknown';
}

function isRetryable(error: unknown): boolean {
  return classifyError(error) === 'retryable' || classifyError(error) === 'unknown';
}
```

Add after imports, before worker definition.
</action>
  <verify>grep -A15 "function classifyError" src/lib/queue/workers/predictions.worker.ts | grep -E "retryable|unrecoverable|timeout|match started"</verify>
  <done>Error classification implemented: retryable (network errors), unrecoverable (business issues), unknown (default retry)</done>
</task>

<task type="auto">
  <name>Task 3: Implement defensive worker processing</name>
  <files>src/lib/queue/workers/predictions.worker.ts</files>
  <action>
Update worker job handler with defensive error handling and isolation:

```typescript
import { PredictionJobData, JobResult } from '../types';
import { logger } from '../../logger';

async function processPredictionJob(job: Job<PredictionJobData>): Promise<JobResult> {
  const log = logger.child({
    module: 'predictions-worker',
    jobId: job.id,
    matchId: job.data.matchId,
    attempt: job.attemptsMade + 1,
  });

  log.info({ matchId: job.data.matchId }, 'Starting prediction job');

  try {
    // Fetch match data with validation
    const matchData = await fetchMatchWithRetry(job.data.matchId, 3, 2000, log);
    if (!matchData || typeof matchData !== 'object') {
      log.warn({ matchId: job.data.matchId }, 'Match not found or invalid data');
      return {
        success: false,
        skipped: true,
        reason: 'match_not_found',
      };
    }

    // Check if match is still valid (not started)
    if (new Date(matchData.date) < new Date()) {
      log.warn({ matchId: job.data.matchId, matchDate: matchData.date }, 'Match already started');
      return {
        success: false,
        skipped: true,
        reason: 'match_started',
      };
    }

    // Get active providers
    const providers = await getActiveProviders();
    const predictions: NewPrediction[] = [];
    const successfulModels: string[] = [];
    const failedModels: Array<{ modelId: string; error: string }> = [];

    // Process each model with isolation
    for (const provider of providers) {
      try {
        const rawResponse = await provider.callAPI(BATCH_SYSTEM_PROMPT, prompt);

        // Validate response before parsing
        if (!rawResponse || typeof rawResponse !== 'string') {
          log.warn({ modelId: provider.id }, 'Provider returned null/invalid response');
          failedModels.push({ modelId: provider.id, error: 'empty_response' });
          continue;
        }

        const parsed = parseBatchPredictionResponse(rawResponse, [job.data.matchId]);

        if (!parsed.success) {
          log.warn({
            modelId: provider.id,
            preview: rawResponse.slice(0, 500),
            error: parsed.error,
          }, 'Parse failed');

          await recordModelFailure(provider.id, parsed.error || 'Parse failed');
          failedModels.push({ modelId: provider.id, error: parsed.error || 'Parse failed' });
          continue;
        }

        predictions.push({
          matchId: job.data.matchId,
          modelId: provider.id,
          predictedHome: parsed.predictions[0].homeScore,
          predictedAway: parsed.predictions[0].awayScore,
          createdAt: new Date(),
        });
        successfulModels.push(provider.id);

      } catch (modelError) {
        const errorMsg = modelError instanceof Error ? modelError.message : String(modelError);
        log.warn({ modelId: provider.id, error: errorMsg }, 'Model prediction failed');
        await recordModelFailure(provider.id, errorMsg);
        failedModels.push({ modelId: provider.id, error: errorMsg });
      }
    }

    // Batch insert successful predictions
    if (predictions.length > 0) {
      await createPredictionsBatch(predictions);

      // Record successes only after successful DB insert
      for (const modelId of successfulModels) {
        await recordModelSuccess(modelId);
      }
    }

    log.info({
      totalModels: providers.length,
      successful: predictions.length,
      failed: failedModels.length,
      failedModels,
    }, 'Prediction job completed');

    return {
      success: true,
      predictionCount: predictions.length,
      failedCount: failedModels.length,
    };

  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error);
    const errorType = classifyError(error);

    log.error({
      error: errorMsg,
      errorType,
      matchId: job.data.matchId,
    }, 'Job failed with exception');

    // Throw for retryable errors (triggers BullMQ retry)
    if (isRetryable(error)) {
      throw new Error(`Retryable: ${errorMsg}`);
    }

    // Return skip result for unrecoverable errors
    return {
      success: false,
      skipped: true,
      reason: 'unrecoverable',
      error: errorMsg,
    };
  }
}
```

This replaces or extends the existing job handler with full defensive processing.
</action>
  <verify>grep -A30 "async function processPredictionJob" src/lib/queue/workers/predictions.worker.ts | grep -E "logger\.child|try {|catch|classifyError"</verify>
  <done>Worker processes predictions with: job logging, match validation, model isolation, error classification, retry/unrecoverable paths</done>
</task>

<task type="auto">
  <name>Task 4: Configure BullMQ worker with backoff strategy</name>
  <files>src/lib/queue/workers/predictions.worker.ts</files>
  <action>
Update worker configuration to use custom backoff strategy:

```typescript
export function createPredictionsWorker() {
  const worker = new Worker<PredictionJobData>(
    QUEUE_NAMES.PREDICTIONS,
    processPredictionJob,
    {
      connection: getQueueConnection(),
      concurrency: 1, // Process one job at a time per worker
      // @ts-ignore - BullMQ backoffStrategy type mismatch
      settings: {
        backoffStrategy: (attemptsMade: number, type: string, err: Error | undefined) => {
          const errorMsg = err?.message || '';

          // Rate limit: 60s fixed backoff
          if (errorMsg.includes('rate limit') || errorMsg.includes('429')) {
            return 60000;
          }

          // Timeout: Linear backoff (5s, 10s, 15s... max 30s)
          if (errorMsg.includes('timeout') || errorMsg.includes('ETIMEDOUT')) {
            return Math.min(attemptsMade * 5000, 30000);
          }

          // Parse errors: Quick retry (5s, 10s, 20s)
          if (errorMsg.includes('parse') || errorMsg.includes('JSON')) {
            return Math.min(5000 * Math.pow(2, attemptsMade), 20000);
          }

          // Default: Exponential backoff with jitter
          const baseDelay = 1000;
          const exponentialDelay = baseDelay * Math.pow(2, attemptsMade);
          const jitter = Math.random() * 0.2; // 20% jitter
          return Math.min(exponentialDelay * (1 + jitter), 60000);
        },
      },
    }
  );

  // Worker event handlers for monitoring
  worker.on('completed', (job) => {
    logger.info({ jobId: job.id, matchId: job.data.matchId }, 'Job completed successfully');
  });

  worker.on('failed', (job, err) => {
    logger.error({
      jobId: job?.id,
      matchId: job?.data.matchId,
      error: err.message,
      attempts: job?.attemptsMade,
    }, 'Job failed');
  });

  worker.on('error', (err) => {
    logger.error({ error: err.message }, 'Worker error');
  });

  return worker;
}
```

Replace or extend existing worker creation function.
</action>
  <verify>grep -A20 "backoffStrategy:" src/lib/queue/workers/predictions.worker.ts | grep -E "rate limit.*60000|timeout.*attempts.*5000|pow\(2"</verify>
  <done>Worker configured with: error-type-aware backoff (rate-limit: 60s, timeout: linear, default: exponential+Jitter), lifecycle event handlers</done>
</task>

</tasks>

<verification>
1. Worker handles null API data: Return {skipped: true, reason: 'match_not_found'}, no crash
2. Error classification working: Simulate timeout error, verify retry logic; simulate "match started", verify skip
3. Model isolation: Mock one model to fail, verify other models still succeed and predictions saved
4. Job logging active: Run a job, check logs for "Starting prediction job" and "Prediction job completed"
5. Failure recovery: Force a retryable error, verify job retries up to 5 times
</verification>

<success_criteria>
- Null/malformed API responses logged and skipped without crashing worker
- Retryable errors (timeout, network) trigger BullMQ retry with appropriate backoff
- Unrecoverable errors (match started) skip job without retry
- Individual model failures don't affect other models in batch (isolation)
- Job lifecycle fully logged (start, success, failure, retry, completed events)
- Worker processes jobs with concurrency=1, each job isolated
</success_criteria>

<output>
After completion, create `.planning/phases/01-critical-stability/01-02-SUMMARY.md`

Use summary template with:
- Error classification implemented (retryable, unrecoverable, unknown)
- Worker isolation pattern applied (per-model try-catch)
- Backoff strategies configured (60s rate limit, linear timeout, exponential default)
- Job lifecycle logging deployed
- Any codebase patterns discovered
- Issues encountered and resolutions
</output>
