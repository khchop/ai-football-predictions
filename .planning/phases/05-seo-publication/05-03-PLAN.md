---
phase: 05-seo-publication
plan: '03'
type: execute
wave: 3
depends_on:
  - '05-01'
files_modified:
  - src/app/matches/[id]/stats/page.tsx
  - src/app/matches/[id]/stats/opengraph-image.tsx
  - src/app/sitemap.ts
  - src/app/robots.ts
autonomous: true
user_setup: []
---

<objective>
Add SEO to stats page and implement sitemap/robots for discoverability.

Purpose: Complete SEO for secondary match page (/matches/{id}/stats) and ensure all match pages are indexed via sitemap and allowed via robots.txt.

Output: Stats page with SEO + sitemap index with match URLs + robots.txt
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/05-seo-publication/05-01-SUMMARY.md
@src/lib/seo/metadata.ts
@src/lib/seo/schema/graph.ts
@src/lib/seo/og/templates.ts
@src/lib/db/queries.ts
</context>

<tasks>

<task type="auto">
  <name>Add dynamic metadata and JSON-LD to stats page</name>
  <files>src/app/matches/[id]/stats/page.tsx</files>
  <action>
Create or update src/app/matches/[id]/stats/page.tsx:
- Import { buildMatchMetadata } from '@/lib/seo/metadata'
- Add export async function generateMetadata({ params }: Props): Promise<Metadata>
- Title: "{Home} vs {Away} - Statistics & Predictions Breakdown"
- Description: "Detailed stats for {Home} vs {Away}. Model prediction accuracy, score distribution, and performance metrics."
- Add JSON-LD injection with SportsEvent schema (reuse from 05-01)
- Set revalidate = 60 for ISR consistency
- Ensure stats-specific schema focus (SportsEvent only, no Article)
</action>
  <verify>
grep -n "generateMetadata\|buildMatchMetadata\|revalidate" src/app/matches/[id]/stats/page.tsx | head -5
cat src/app/matches/[id]/stats/page.tsx | grep "application/ld+json"</verify>
  <done>
Stats page has dynamic metadata and JSON-LD structured data</done>
</task>

<task type="auto">
  <name>Create OG image for stats page</name>
  <files>src/app/matches/[id]/stats/opengraph-image.tsx</files>
  <action>
Create src/app/matches/[id]/stats/opengraph-image.tsx:
- Export const alt = 'Match Statistics'
- Export const size = { width: 1200, height: 630 }
- Create default async function Image({ params }: Props):
  - Fetches match data
  - Template similar to main match page but stats-focused
  - Shows: team logos, score, "Statistics & Predictions" badge
  - Include data visualization hints (charts/bars) if possible
  - Use lighter background for stats context
</action>
  <verify>
ls -la src/app/matches/[id]/stats/opengraph-image.tsx
cat src/app/matches/[id]/stats/opengraph-image.tsx | grep "export const\|ImageResponse"</verify>
  <done>
Stats-specific OG image generated with statistics branding</done>
</task>

<task type="auto">
  <name>Implement sitemap generation with chunking</name>
  <files>src/app/sitemap.ts</files>
  <action>
Create src/app/sitemap.ts:
- Export async function generateSitemaps(): Returns array of { id: string } for pagination
- Export default async function sitemap({ params }: Props): Returns MetadataRoute.Sitemap
- Base pages: /, /matches, /leaderboard, /models with appropriate priority
- Match URLs: Fetch paginated matches from database, return with:
  - url: `${BASE_URL}/matches/${match.id}`
  - lastModified: match.updatedAt
  - changeFrequency: 'daily' (or 'hourly' for live matches)
  - priority: 0.9 (live), 0.8 (upcoming), 0.7 (finished)
- Chunk size: 50,000 URLs per sitemap as per Next.js limits
</action>
  <verify>
cat src/app/sitemap.ts | grep "generateSitemaps\|sitemap\|changeFrequency"
curl -s http://localhost:3000/sitemap.xml 2>/dev/null | head -20</verify>
  <done>
Sitemap generated with base pages and all match URLs</done>
</task>

<task type="auto">
  <name>Create robots.txt configuration</name>
  <files>src/app/robots.ts</files>
  <action>
Create src/app/robots.ts:
- Export default function robots(): Returns MetadataRoute.Robots
- Allow: /matches/
- Allow: /leaderboard
- Allow: /models
- Disallow: /api/
- Disallow: /admin/
- Sitemap: `${BASE_URL}/sitemap.xml`
- Host: BASE_URL
</action>
  <verify>
cat src/app/robots.ts
curl -s http://localhost:3000/robots.txt 2>/dev/null</verify>
  <done>
Robots.txt allows crawling of public pages, disallows admin/api</done>
</task>

</tasks>

<verification>
1. Stats page SEO: curl | grep -E '<title>|<meta name="description"'
2. Stats OG image: curl -I /matches/{id}/stats/opengraph-image returns 200
3. Sitemap accessible: curl /sitemap.xml shows index with match URLs
4. Robots.txt accessible: curl /robots.txt shows correct rules
5. JSON-LD in stats page: Check for SportsEvent schema without Article
</verification>

<success_criteria>
1. Stats page has unique metadata (SEO-01, SEO-02)
2. Stats page has JSON-LD SportsEvent schema (SEO-03)
3. All match pages discoverable via sitemap (SEO-04)
4. Search engines can crawl public pages (robots.txt)
5. Sitemap chunks handle large URL sets efficiently
</success_criteria>

<output>
After completion, create `.planning/phases/05-seo-publication/05-03-SUMMARY.md`
</output>
