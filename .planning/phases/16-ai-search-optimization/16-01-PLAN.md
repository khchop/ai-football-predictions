---
phase: 16-ai-search-optimization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/robots.ts
autonomous: true

must_haves:
  truths:
    - "GPTBot can access all match pages"
    - "ClaudeBot can access all match pages"
    - "PerplexityBot can access all match pages"
    - "Amazonbot can access all match pages"
    - "OAI-SearchBot can access all match pages"
  artifacts:
    - path: "src/app/robots.ts"
      provides: "AI crawler allow rules"
      contains: "Amazonbot"
  key_links:
    - from: "robots.txt output"
      to: "AI crawler user-agents"
      via: "Allow rules for each crawler"
      pattern: "userAgent.*Amazonbot|OAI-SearchBot|Claude-SearchBot"
---

<objective>
Add missing AI crawler user-agents to robots.txt for comprehensive AI search engine access.

Purpose: Ensure all major AI search engines (ChatGPT Search, Claude Search, Perplexity, Amazon Q) can crawl and index match pages for citation in AI-generated search results.

Output: Updated robots.ts with explicit allow rules for Amazonbot, OAI-SearchBot, and Claude-SearchBot.
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/16-ai-search-optimization/16-RESEARCH.md
@src/app/robots.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add missing AI crawler user-agents to robots.ts</name>
  <files>src/app/robots.ts</files>
  <action>
Update src/app/robots.ts to add explicit allow rules for missing AI crawlers:

1. Add OAI-SearchBot (OpenAI's search crawler, distinct from GPTBot training crawler)
2. Add Claude-SearchBot (Anthropic's search crawler, distinct from ClaudeBot training crawler)
3. Add Amazonbot (Amazon's crawler for Alexa, Amazon Q, and shopping search)

Keep existing crawlers (GPTBot, ChatGPT-User, Google-Extended, PerplexityBot, ClaudeBot, Anthropic-AI, CCBot).

Order rules consistently: general rule first, then AI crawlers grouped by company (OpenAI, Anthropic, Google, Perplexity, Amazon, Common Crawl).

Add a comment block at the top explaining the AI crawler strategy for future maintainability.
  </action>
  <verify>
Run: `curl -s https://kroam.xyz/robots.txt` (after deploy) or check generated output locally via Next.js dev server.

Expected: robots.txt contains separate `User-agent:` blocks for:
- GPTBot, ChatGPT-User, OAI-SearchBot (OpenAI)
- ClaudeBot, Claude-SearchBot, Anthropic-AI (Anthropic)
- Google-Extended (Google)
- PerplexityBot (Perplexity)
- Amazonbot (Amazon)
- CCBot (Common Crawl)

Each with `Allow: /` directive.
  </verify>
  <done>
robots.ts includes Amazonbot, OAI-SearchBot, and Claude-SearchBot allow rules. All 10+ AI crawler user-agents explicitly allowed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify llms.txt structure and update timestamp handling</name>
  <files>src/app/llms.txt/route.ts</files>
  <action>
Review existing llms.txt implementation and verify it meets spec requirements:

1. Verify H1 site name exists (# kroam.xyz)
2. Verify blockquote summary exists (> An AI-powered...)
3. Verify H2 sections with descriptive content exist
4. Verify Important URLs section includes sitemap, robots.txt, homepage

If all correct (which research indicates), make one small improvement:
- Add explicit URL markdown links in the "Important URLs" section using proper markdown link syntax: `[Sitemap](https://kroam.xyz/sitemap.xml)` instead of bare URLs

This improves AI parser compatibility since some LLMs parse markdown links better than bare URLs.

Do NOT change the dynamic model count logic or caching strategy - those work correctly.
  </action>
  <verify>
Run: `curl -s https://kroam.xyz/llms.txt` (after deploy) or check locally.

Expected:
- H1 header present
- Blockquote summary present
- Important URLs section has markdown links (not bare URLs)
- Content-Type is text/plain
  </verify>
  <done>
llms.txt follows llmstxt.org specification with markdown links in Important URLs section.
  </done>
</task>

</tasks>

<verification>
1. `npm run build` completes without errors
2. robots.ts output includes all AI crawler user-agents
3. llms.txt output follows specification structure
</verification>

<success_criteria>
- [ ] robots.ts includes Amazonbot, OAI-SearchBot, Claude-SearchBot allow rules
- [ ] All 10+ major AI crawlers explicitly allowed in robots.txt
- [ ] llms.txt uses markdown link syntax in Important URLs section
- [ ] Build passes without errors
</success_criteria>

<output>
After completion, create `.planning/phases/16-ai-search-optimization/16-01-SUMMARY.md`
</output>
