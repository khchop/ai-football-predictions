---
phase: 41-together-ai-fallbacks
plan: 02
type: execute
wave: 2
depends_on: ["41-01"]
files_modified:
  - src/lib/llm/providers/base.ts
  - src/lib/queue/workers/predictions.worker.ts
autonomous: true

must_haves:
  truths:
    - "Synthetic model failures trigger automatic fallback to Together AI"
    - "Fallback returns tuple with response and usedFallback boolean"
    - "Predictions track usedFallback status in database"
    - "If fallback also fails, prediction fails (max depth 1)"
  artifacts:
    - path: "src/lib/llm/providers/base.ts"
      provides: "callAPIWithFallback method"
      contains: "callAPIWithFallback"
    - path: "src/lib/queue/workers/predictions.worker.ts"
      provides: "Fallback integration in prediction loop"
      contains: "usedFallback"
  key_links:
    - from: "src/lib/llm/providers/base.ts"
      to: "getFallbackProvider"
      via: "import and use for fallback lookup"
      pattern: "getFallbackProvider"
    - from: "src/lib/queue/workers/predictions.worker.ts"
      to: "callAPIWithFallback"
      via: "calls wrapper instead of raw callAPI"
      pattern: "callAPIWithFallback"
---

<objective>
Implement fallback orchestration wrapper and integrate into predictions worker.

Purpose: When a Synthetic model fails (any error: timeout, parse error, API error), automatically retry with the Together AI equivalent. Track fallback usage in prediction records for admin visibility.

Output:
- callAPIWithFallback() method on BaseLLMProvider
- Predictions worker uses fallback wrapper
- usedFallback populated in prediction records
</objective>

<execution_context>
@/Users/pieterbos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pieterbos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/41-together-ai-fallbacks/41-CONTEXT.md
@.planning/phases/41-together-ai-fallbacks/41-RESEARCH.md
@.planning/phases/41-together-ai-fallbacks/41-01-SUMMARY.md

# Key files to reference
@src/lib/llm/providers/base.ts
@src/lib/llm/index.ts
@src/lib/queue/workers/predictions.worker.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add callAPIWithFallback method to base provider</name>
  <files>src/lib/llm/providers/base.ts</files>
  <action>
Add a new method to OpenAICompatibleProvider class that wraps callAPI with fallback logic:

```typescript
// Import at top of file
import { getFallbackProvider } from '../index';

// Add this interface for the fallback result
export interface FallbackAPIResult {
  response: string;
  usedFallback: boolean;
}

// Add this method to OpenAICompatibleProvider class (after callAPI method)
/**
 * Call API with automatic fallback to Together AI on failure
 *
 * User decisions (from CONTEXT.md):
 * - Any error triggers fallback (timeout, parse error, empty response, API error, rate limit)
 * - No retries on original model - first failure immediately triggers fallback
 * - If fallback also fails, throw error (max depth 1, no fallback chains)
 * - Attribution: return original model's response context, track fallback internally
 */
async callAPIWithFallback(
  systemPrompt: string,
  userPrompt: string
): Promise<FallbackAPIResult> {
  try {
    // Try original model first
    const response = await this.callAPI(systemPrompt, userPrompt);
    return { response, usedFallback: false };
  } catch (originalError) {
    // Get fallback provider for this model
    const fallbackProvider = getFallbackProvider(this.id);

    if (!fallbackProvider) {
      // No fallback available, propagate original error
      throw originalError;
    }

    // Log fallback attempt
    logger.warn({
      originalModel: this.id,
      fallbackModel: fallbackProvider.id,
      error: originalError instanceof Error ? originalError.message : String(originalError),
    }, 'Model failed, attempting fallback to Together AI');

    try {
      // Try fallback provider (direct callAPI call, NOT recursive)
      // Cast needed because getFallbackProvider returns LLMProvider interface
      const fallbackResult = await (fallbackProvider as OpenAICompatibleProvider).callAPI(
        systemPrompt,
        userPrompt
      );

      logger.info({
        originalModel: this.id,
        fallbackModel: fallbackProvider.id,
      }, 'Fallback succeeded');

      return { response: fallbackResult, usedFallback: true };
    } catch (fallbackError) {
      // Fallback also failed - max depth 1, no more retries
      logger.error({
        originalModel: this.id,
        fallbackModel: fallbackProvider.id,
        originalError: originalError instanceof Error ? originalError.message : String(originalError),
        fallbackError: fallbackError instanceof Error ? fallbackError.message : String(fallbackError),
      }, 'Both original and fallback models failed');

      // Throw the fallback error (more recent/relevant)
      throw fallbackError;
    }
  }
}
```

Key implementation notes:
1. Method is on OpenAICompatibleProvider, not BaseLLMProvider, because it uses this.callAPI which is abstract on base
2. No cycle detection needed - getFallbackProvider only returns Together AI providers, which have no fallbacks defined
3. No depth tracking needed - max depth 1 is enforced structurally (Together providers have no fallbacks)
4. Cast to OpenAICompatibleProvider is safe because all providers in ALL_PROVIDERS extend it
  </action>
  <verify>TypeScript compilation passes: `npx tsc --noEmit`</verify>
  <done>callAPIWithFallback method exists on OpenAICompatibleProvider, returns FallbackAPIResult tuple</done>
</task>

<task type="auto">
  <name>Task 2: Integrate fallback into predictions worker</name>
  <files>src/lib/queue/workers/predictions.worker.ts</files>
  <action>
Update the predictions worker to use callAPIWithFallback and track usedFallback in prediction records.

1. Import the FallbackAPIResult type:
```typescript
import { FallbackAPIResult } from '@/lib/llm/providers/base';
```

2. Change the API call (around line 162) from:
```typescript
const rawResponse = await (provider as unknown as { callAPI: (system: string, user: string) => Promise<string> }).callAPI(BATCH_SYSTEM_PROMPT, prompt);
```

To:
```typescript
// Use fallback-aware API call
const apiResult = await (provider as unknown as {
  callAPIWithFallback: (system: string, user: string) => Promise<FallbackAPIResult>
}).callAPIWithFallback(BATCH_SYSTEM_PROMPT, prompt);

const rawResponse = apiResult.response;
const usedFallback = apiResult.usedFallback;
```

3. Update the prediction object creation (around line 198) to include usedFallback:
```typescript
predictionsToInsert.push({
  id: uuidv4(),
  matchId,
  modelId: provider.id,  // Original model (user-facing attribution)
  predictedHome: prediction.homeScore,
  predictedAway: prediction.awayScore,
  predictedResult: result,
  status: 'pending',
  usedFallback,  // NEW: Track fallback usage
});
```

4. Update the success log (around line 210) to include fallback info:
```typescript
log.info({
  matchId,
  modelId: provider.id,
  prediction: `${prediction.homeScore}-${prediction.awayScore}`,
  usedFallback,  // NEW
}, `${usedFallback ? '↩' : '✓'} Prediction generated${usedFallback ? ' (via fallback)' : ''}`);
```

Note: The usedFallback variable is scoped to each provider iteration, so each prediction correctly tracks its own fallback status.
  </action>
  <verify>
1. TypeScript compilation passes: `npx tsc --noEmit`
2. Worker can be instantiated: Check that importing predictions.worker.ts doesn't throw
  </verify>
  <done>Predictions worker calls callAPIWithFallback and populates usedFallback in NewPrediction objects</done>
</task>

<task type="auto">
  <name>Task 3: Update NewPrediction type to include usedFallback</name>
  <files>src/lib/db/schema.ts</files>
  <action>
Verify that the NewPrediction type correctly includes usedFallback after Task 1 changes.

The type is auto-inferred from the schema:
```typescript
export type NewPrediction = typeof predictions.$inferInsert;
```

Since usedFallback has a default value, it should be optional in NewPrediction (usedFallback?: boolean).

Run TypeScript to verify the type includes usedFallback:
```bash
npx tsc --noEmit
```

If there are type errors in predictions.worker.ts about usedFallback not existing on NewPrediction, it means the schema change wasn't picked up. In that case:
1. Check that the schema.ts file was saved
2. Restart the TypeScript server (if using IDE)
3. Run `npx tsc --noEmit` to verify

No code changes should be needed if Task 1 was done correctly - this task is verification only.
  </action>
  <verify>`npx tsc --noEmit` passes with no errors related to usedFallback type</verify>
  <done>NewPrediction type includes optional usedFallback property inferred from schema</done>
</task>

</tasks>

<verification>
1. Type verification:
   - `npx tsc --noEmit` passes
   - FallbackAPIResult type is exported from base.ts
   - NewPrediction includes usedFallback

2. Logic verification:
   - callAPIWithFallback tries original model first
   - On failure, looks up fallback provider
   - If no fallback, throws original error
   - If fallback exists, tries it
   - If fallback succeeds, returns usedFallback: true
   - If fallback fails, throws fallback error

3. Integration verification:
   - Predictions worker imports FallbackAPIResult
   - Worker calls callAPIWithFallback instead of callAPI
   - Worker includes usedFallback in prediction records
</verification>

<success_criteria>
- callAPIWithFallback method returns { response, usedFallback } tuple
- Synthetic model failures trigger fallback to Together AI
- Fallback failures throw error (max depth 1)
- Predictions include usedFallback boolean in database
- Logs indicate when fallback is used
</success_criteria>

<output>
After completion, create `.planning/phases/41-together-ai-fallbacks/41-02-SUMMARY.md`
</output>
